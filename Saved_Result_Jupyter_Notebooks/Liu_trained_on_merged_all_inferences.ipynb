{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3254f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.2+cu121\n",
      "Torchvision Version:  0.16.2+cu121\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torchvision import models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81203cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "   '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "   '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP','.mat',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "   return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "   classes = os.listdir(dir)\n",
    "   classes.sort()\n",
    "   class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "   return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "   images = []\n",
    "   for target in os.listdir(dir):\n",
    "       d = os.path.join(dir, target)\n",
    "       if not os.path.isdir(d):\n",
    "           continue\n",
    "\n",
    "       for filename in os.listdir(d):\n",
    "           if is_image_file(filename):\n",
    "               path = '{0}/{1}'.format(target, filename)\n",
    "               #print(path)\n",
    "               item = (path, class_to_idx[target])\n",
    "               images.append(item)\n",
    "\n",
    "   return images\n",
    "\n",
    "def default_loader(path):\n",
    "   return Image.open(path).convert('RGB')\n",
    "\n",
    "def mat_loader(path):\n",
    "   return scipy.io.loadmat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d42d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 133120\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 122880\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 256000\n"
     ]
    }
   ],
   "source": [
    "classes1, class_to_idx1 = find_classes(\"/home/user1/icip/sim_Liu/train/\")\n",
    "       \n",
    "imgs1 = make_dataset(\"/home/user1/icip/random_Liu/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/sim_Liu/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/Merged_Liu/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a611827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderLoader(data.Dataset):\n",
    "   def __init__(self, root1,transform_1=None,\n",
    "                target_transform=None,\n",
    "                loader=default_loader):\n",
    "       classes1, class_to_idx1 = find_classes(root1)\n",
    "       \n",
    "       imgs1 = make_dataset(root1, class_to_idx1)\n",
    "      \n",
    "\n",
    "       self.root1 = root1\n",
    "       self.imgs1 = imgs1\n",
    "       self.classes1 = classes1\n",
    "       self.class_to_idx1 = class_to_idx1\n",
    "       self.target_transform = target_transform\n",
    "       self.loader = loader\n",
    "       self.img_transform = transform_1\n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "   def __getitem__(self, index):\n",
    "    \n",
    "\n",
    "       path1, target1 = self.imgs1[index]\n",
    "       filename = Path(path1).stem \n",
    "       \n",
    "    \n",
    "       img1 = self.loader(os.path.join(self.root1, path1))  \n",
    "       \n",
    "       if self.img_transform is not None:\n",
    "           img1 = self.img_transform(img1)\n",
    "        \n",
    "       if self.target_transform is not None:\n",
    "           target1 = self.target_transform(target)\n",
    "        \n",
    "       target1 = torch.eye(30)[target1]      \n",
    "            \n",
    "       return img1,target1,filename\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.imgs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2e60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batchsize=1\n",
    "\n",
    "val_dataset_random = ImageFolderLoader(\n",
    "        \"/home/user1/icip/random_Liu/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_random = torch.utils.data.DataLoader(\n",
    "        val_dataset_random, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_similar = ImageFolderLoader(\n",
    "        \"/home/user1/icip/sim_Liu/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_similar = torch.utils.data.DataLoader(\n",
    "        val_dataset_similar, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_merged = ImageFolderLoader(\n",
    "        \"/home/user1/icip/Merged_Liu/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_merged = torch.utils.data.DataLoader(\n",
    "        val_dataset_merged, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6ae654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133120, 122880, 256000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset_random), len(val_dataset_similar), len(val_dataset_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9832e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        super(Res2Net, self).__init__()\n",
    "        inplanes = 3\n",
    "        planes = 3\n",
    "        \n",
    "        width = 16\n",
    "        self.width_1 = 16\n",
    "        scale = 4\n",
    "        stride = 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width*scale)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "       # self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.nums = scale\n",
    "        \n",
    "        self.conv11 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn11 = nn.BatchNorm2d(width)\n",
    "        self.relu11 = nn.ReLU(inplace=True)\n",
    "        self.conv12 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn12 = nn.BatchNorm2d(width)\n",
    "        self.relu12 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "        self.conv21 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn21 = nn.BatchNorm2d(width)\n",
    "        self.relu21 = nn.ReLU(inplace=True)\n",
    "        self.conv22 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn22 = nn.BatchNorm2d(width)\n",
    "        self.relu22 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv31 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn31 = nn.BatchNorm2d(width)\n",
    "        self.relu31 = nn.ReLU(inplace=True)\n",
    "        self.conv32 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn32 = nn.BatchNorm2d(width)\n",
    "        self.relu32 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn41 = nn.BatchNorm2d(width)\n",
    "        self.relu41 = nn.ReLU(inplace=True)\n",
    "        self.conv42 = nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False)\n",
    "        self.bn42 = nn.BatchNorm2d(width)\n",
    "        self.relu42 = nn.ReLU(inplace=True)\n",
    "        \n",
    "          #convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n",
    "          #bns.append(nn.BatchNorm2d(width))\n",
    "          #convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n",
    "         \n",
    "            \n",
    "            \n",
    "        #self.convs = nn.ModuleList(convs)\n",
    "        #self.bns = nn.ModuleList(bns)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width*scale, planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        vgg1 =  models.vgg16(pretrained=False)\n",
    "        modules1 = list(vgg1.children())[:-2]      # delete the last fc layer.\n",
    "        self.vgg = nn.Sequential(*modules1)\n",
    "        \n",
    "        self.globpool = nn.AvgPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 30)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        spx = torch.split(x, self.width_1, 1)\n",
    "        \n",
    "        x1 = self.conv11(spx[0])\n",
    "        x1 = self.bn11(x1)\n",
    "        x1 = self.relu11(x1)\n",
    "        x1 = self.conv12(x1)\n",
    "        x1 = self.bn12(x1)\n",
    "        x1 = self.relu12(x1)\n",
    "        \n",
    "        x2 = x1 + spx[1]\n",
    "        x2 = self.conv21(x2)\n",
    "        x2 = self.bn21(x2)\n",
    "        x2 = self.relu21(x2)\n",
    "        x2 = self.conv22(x2)\n",
    "        x2 = self.bn22(x2)\n",
    "        x2 = self.relu22(x2)\n",
    "        \n",
    "        x3 = x2 + spx[2]\n",
    "        x3 = self.conv31(x3)\n",
    "        x3 = self.bn31(x3)\n",
    "        x3 = self.relu31(x3)\n",
    "        x3 = self.conv32(x3)\n",
    "        x3 = self.bn32(x3)\n",
    "        x3 = self.relu32(x3)\n",
    "        \n",
    "        x4 = x3 + spx[3]\n",
    "        x4 = self.conv41(x4)\n",
    "        x4 = self.bn41(x4)\n",
    "        x4 = self.relu41(x4)\n",
    "        x4 = self.conv22(x4)\n",
    "        x4 = self.bn42(x4)\n",
    "        x4 = self.relu42(x4)\n",
    "        \n",
    "        \n",
    "              \n",
    "        out = torch.cat((x1,x2,x3,x4), 1)\n",
    "                     \n",
    "        \n",
    "        x = self.conv3(out)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = x - residual\n",
    "        \n",
    "        x = self.vgg(x)\n",
    "        \n",
    "        x = self.globpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.softmax(x)    \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf57dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')\n",
    "model = Res2Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6246990b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_rnd_34_15',) 0 0 0.99\n",
      "1000 ('D01_rnd_130_89',) 0 0 0.82\n",
      "2000 ('D01_rnd_110_123',) 0 0 1.0\n",
      "3000 ('D01_rnd_99_82',) 0 15 0.99\n",
      "4000 ('D01_rnd_74_78',) 0 0 0.99\n",
      "Yes_Class 28\n",
      "5000 ('D29_rnd_135_46',) 28 28 0.5\n",
      "6000 ('D29_rnd_113_27',) 28 28 1.0\n",
      "7000 ('D29_rnd_108_101',) 28 28 0.99\n",
      "8000 ('D29_rnd_113_57',) 28 28 1.0\n",
      "Yes_Class 22\n",
      "9000 ('D23_rnd_25_88',) 22 29 0.29\n",
      "10000 ('D23_rnd_135_21',) 22 22 0.94\n",
      "11000 ('D23_rnd_138_35',) 22 22 1.0\n",
      "12000 ('D23_rnd_130_116',) 22 22 0.64\n",
      "Yes_Class 27\n",
      "13000 ('D28_rnd_69_108',) 27 5 0.74\n",
      "14000 ('D28_rnd_99_47',) 27 5 0.54\n",
      "15000 ('D28_rnd_138_45',) 27 27 0.98\n",
      "16000 ('D28_rnd_81_0',) 27 27 0.95\n",
      "Yes_Class 3\n",
      "17000 ('D04_rnd_240_20',) 3 3 1.0\n",
      "18000 ('D04_rnd_215_34',) 3 3 1.0\n",
      "19000 ('D04_rnd_197_12',) 3 3 1.0\n",
      "20000 ('D04_rnd_8_82',) 3 3 1.0\n",
      "21000 ('D04_rnd_215_74',) 3 3 1.0\n",
      "22000 ('D04_rnd_160_23',) 3 3 1.0\n",
      "23000 ('D04_rnd_240_113',) 3 3 1.0\n",
      "Yes_Class 14\n",
      "24000 ('D15_rnd_41_87',) 14 14 1.0\n",
      "25000 ('D15_rnd_111_74',) 14 14 1.0\n",
      "26000 ('D15_rnd_101_31',) 14 2 1.0\n",
      "27000 ('D15_rnd_255_66',) 14 14 1.0\n",
      "28000 ('D15_rnd_10_72',) 14 14 0.99\n",
      "29000 ('D15_rnd_104_71',) 14 14 0.68\n",
      "30000 ('D15_rnd_122_74',) 14 10 0.77\n",
      "Yes_Class 20\n",
      "31000 ('D21_rnd_130_94',) 20 20 0.55\n",
      "32000 ('D21_rnd_122_14',) 20 20 0.79\n",
      "33000 ('D21_rnd_81_118',) 20 20 0.76\n",
      "34000 ('D21_rnd_122_54',) 20 15 0.64\n",
      "Yes_Class 4\n",
      "35000 ('D05_rnd_87_112',) 4 4 0.76\n",
      "36000 ('D05_rnd_122_4',) 4 4 0.91\n",
      "37000 ('D05_rnd_119_59',) 4 4 0.86\n",
      "38000 ('D05_rnd_141_92',) 4 4 0.96\n",
      "Yes_Class 18\n",
      "39000 ('D19_rnd_146_71',) 18 18 0.98\n",
      "40000 ('D19_rnd_5_19',) 18 29 0.31\n",
      "41000 ('D19_rnd_197_12',) 18 18 1.0\n",
      "42000 ('D19_rnd_99_115',) 18 18 0.99\n",
      "43000 ('D19_rnd_138_113',) 18 18 0.86\n",
      "44000 ('D19_rnd_187_2',) 18 18 0.99\n",
      "Yes_Class 17\n",
      "45000 ('D18_rnd_122_33',) 17 17 0.89\n",
      "46000 ('D18_rnd_85_103',) 17 24 0.49\n",
      "47000 ('D18_rnd_30_17',) 17 17 0.99\n",
      "48000 ('D18_rnd_85_109',) 17 17 0.91\n",
      "Yes_Class 10\n",
      "49000 ('D11_rnd_84_125',) 10 10 0.97\n",
      "50000 ('D11_rnd_49_32',) 10 10 1.0\n",
      "51000 ('D11_rnd_10_72',) 10 10 0.62\n",
      "52000 ('D11_rnd_96_62',) 10 10 0.45\n",
      "Yes_Class 12\n",
      "53000 ('D13_rnd_135_36',) 12 12 0.99\n",
      "54000 ('D13_rnd_117_37',) 12 12 0.56\n",
      "55000 ('D13_rnd_153_95',) 12 12 0.81\n",
      "56000 ('D13_rnd_141_74',) 12 12 0.99\n",
      "57000 ('D13_rnd_122_113',) 12 11 0.7\n",
      "Yes_Class 19\n",
      "58000 ('D20_rnd_124_54',) 19 1 0.17\n",
      "59000 ('D20_rnd_85_41',) 19 19 1.0\n",
      "60000 ('D20_rnd_99_46',) 19 19 1.0\n",
      "61000 ('D20_rnd_135_88',) 19 19 0.99\n",
      "Yes_Class 1\n",
      "62000 ('D02_rnd_108_23',) 1 1 0.96\n",
      "63000 ('D02_rnd_119_85',) 1 1 1.0\n",
      "64000 ('D02_rnd_187_95',) 1 1 0.93\n",
      "65000 ('D02_rnd_110_108',) 1 24 0.47\n",
      "66000 ('D02_rnd_197_27',) 1 20 0.54\n",
      "Yes_Class 2\n",
      "67000 ('D03_rnd_99_76',) 2 2 1.0\n",
      "68000 ('D03_rnd_146_119',) 2 2 1.0\n",
      "69000 ('D03_rnd_138_23',) 2 2 0.99\n",
      "70000 ('D03_rnd_166_22',) 2 2 1.0\n",
      "71000 ('D03_rnd_176_106',) 2 2 1.0\n",
      "72000 ('D03_rnd_52_60',) 2 2 1.0\n",
      "Yes_Class 21\n",
      "73000 ('D22_rnd_39_96',) 21 21 0.77\n",
      "74000 ('D22_rnd_113_67',) 21 21 1.0\n",
      "75000 ('D22_rnd_50_114',) 21 21 1.0\n",
      "76000 ('D22_rnd_130_58',) 21 21 0.73\n",
      "Yes_Class 23\n",
      "77000 ('D24_rnd_44_4',) 23 23 1.0\n",
      "78000 ('D24_rnd_84_116',) 23 23 0.97\n",
      "79000 ('D24_rnd_42_120',) 23 23 1.0\n",
      "80000 ('D24_rnd_44_66',) 23 23 1.0\n",
      "81000 ('D24_rnd_45_102',) 23 23 1.0\n",
      "Yes_Class 25\n",
      "82000 ('D26_rnd_138_71',) 25 25 0.98\n",
      "83000 ('D26_rnd_138_69',) 25 25 0.8\n",
      "84000 ('D26_rnd_3_107',) 25 25 0.98\n",
      "85000 ('D26_rnd_135_102',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "86000 ('D10_rnd_83_118',) 9 9 1.0\n",
      "87000 ('D10_rnd_124_104',) 9 9 1.0\n",
      "88000 ('D10_rnd_150_47',) 9 9 1.0\n",
      "89000 ('D10_rnd_138_54',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "90000 ('D30_rnd_124_22',) 29 29 1.0\n",
      "91000 ('D30_rnd_130_61',) 29 29 1.0\n",
      "92000 ('D30_rnd_130_25',) 29 29 1.0\n",
      "93000 ('D30_rnd_85_73',) 29 29 1.0\n",
      "Yes_Class 7\n",
      "94000 ('D08_rnd_96_3',) 7 7 0.99\n",
      "95000 ('D08_rnd_99_127',) 7 7 1.0\n",
      "96000 ('D08_rnd_147_42',) 7 7 1.0\n",
      "97000 ('D08_rnd_187_7',) 7 7 1.0\n",
      "98000 ('D08_rnd_26_17',) 7 7 1.0\n",
      "Yes_Class 13\n",
      "99000 ('D14_rnd_129_50',) 13 13 0.74\n",
      "100000 ('D14_rnd_18_25',) 13 0 0.99\n",
      "101000 ('D14_rnd_160_34',) 13 13 0.97\n",
      "102000 ('D14_rnd_86_27',) 13 4 0.2\n",
      "103000 ('D14_rnd_146_35',) 13 4 0.22\n",
      "Yes_Class 6\n",
      "104000 ('D07_rnd_10_52',) 6 5 0.34\n",
      "105000 ('D07_rnd_96_69',) 6 6 0.83\n",
      "106000 ('D07_rnd_78_108',) 6 6 1.0\n",
      "107000 ('D07_rnd_117_16',) 6 6 0.56\n",
      "Yes_Class 16\n",
      "108000 ('D17_rnd_130_121',) 16 16 1.0\n",
      "109000 ('D17_rnd_113_83',) 16 16 0.99\n",
      "110000 ('D17_rnd_135_122',) 16 16 1.0\n",
      "111000 ('D17_rnd_16_6',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "112000 ('D27_rnd_8_82',) 26 26 1.0\n",
      "113000 ('D27_rnd_89_11',) 26 26 0.99\n",
      "114000 ('D27_rnd_138_99',) 26 26 1.0\n",
      "115000 ('D27_rnd_44_56',) 26 26 0.99\n",
      "Yes_Class 11\n",
      "116000 ('D12_rnd_113_107',) 11 11 0.98\n",
      "117000 ('D12_rnd_188_53',) 11 11 0.79\n",
      "118000 ('D12_rnd_36_40',) 11 11 1.0\n",
      "119000 ('D12_rnd_26_37',) 11 11 0.94\n",
      "120000 ('D12_rnd_187_83',) 11 11 0.75\n",
      "Yes_Class 15\n",
      "121000 ('D16_rnd_55_77',) 15 15 0.99\n",
      "122000 ('D16_rnd_129_74',) 15 15 0.63\n",
      "123000 ('D16_rnd_74_20',) 15 15 1.0\n",
      "124000 ('D16_rnd_138_119',) 15 15 0.67\n",
      "Yes_Class 24\n",
      "125000 ('D25_rnd_49_58',) 24 24 0.9\n",
      "126000 ('D25_rnd_67_89',) 24 24 0.84\n",
      "127000 ('D25_rnd_52_96',) 24 24 0.94\n",
      "128000 ('D25_rnd_129_45',) 24 13 0.55\n",
      "Yes_Class 8\n",
      "129000 ('D09_rnd_45_70',) 8 8 1.0\n",
      "130000 ('D09_rnd_135_34',) 8 8 0.98\n",
      "131000 ('D09_rnd_35_47',) 8 8 1.0\n",
      "132000 ('D09_rnd_108_31',) 8 8 1.0\n",
      "133000 ('D09_rnd_124_43',) 8 8 0.98\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"Liu_trained_model_merged_final\")\n",
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Random__\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Random__ folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a=-1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1, labels1,patch_filename) in enumerate(test_loader_random):\n",
    "        _, c = torch.max(labels1.data,1)\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org,target = imgs1.to(device,dtype=torch.float), labels1.to(device)\n",
    "        #img_org = img_org.permute(0, 3, 1, 2)\n",
    "        \n",
    "        output = model(img_org)\n",
    "        \n",
    "\n",
    "        _, actual = torch.max(target.data, 1)    \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601a0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Random__/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results_Random.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a47c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 4608 33 3363 79.62 0.85\n",
      "32 4096 32 3739 91.28 0.92\n",
      "43 5504 42 5080 94.49 0.97\n",
      "38 4864 34 4020 92.37 0.88\n",
      "32 4096 31 3568 89.92 0.95\n",
      "34 4352 34 3890 89.38 0.84\n",
      "32 4096 30 3773 98.26 0.99\n",
      "40 5120 38 4779 98.25 0.98\n",
      "44 5632 43 4906 89.14 0.91\n",
      "47 6016 46 5463 92.78 0.96\n",
      "35 4480 29 2935 79.07 0.85\n",
      "31 3968 31 3562 89.77 0.92\n",
      "34 4352 33 3679 87.1 0.92\n",
      "34 4352 32 3606 88.04 0.93\n",
      "33 4224 31 3772 95.06 0.97\n",
      "31 3968 30 3102 80.78 0.8\n",
      "34 4352 32 3334 81.4 0.83\n",
      "31 3968 26 2223 66.8 0.81\n",
      "31 3968 30 3641 94.82 0.98\n",
      "56 7168 49 5659 90.23 0.92\n",
      "51 6528 49 6181 98.55 0.99\n",
      "33 4224 32 3531 86.21 0.89\n",
      "33 4224 33 4118 97.49 0.99\n",
      "31 3968 18 1752 76.04 0.76\n",
      "30 3840 29 3269 88.07 0.93\n",
      "32 4096 32 3538 86.38 0.94\n",
      "32 4096 32 4013 97.97 0.98\n",
      "35 4480 34 4097 94.14 0.93\n",
      "35 4480 33 3817 90.36 0.92\n",
      "Image Level Accuracy: 94.03846153846153%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    data = df.sort_values(by=['Patch_Filename'])\n",
    "    classname = Path(f).stem\n",
    "    classname = int(classname.split(\"_\")[2])\n",
    "    a=\"a\"\n",
    "\n",
    "    true_image_class = classname\n",
    "    total_images_perclass = 0\n",
    "    correct_images_perclass = 0\n",
    "    total_patches_perclass = 0\n",
    "\n",
    "    total_class_votes =0\n",
    "    total_class_patches =0\n",
    "    prob_avg_correct_patch = 0.0\n",
    "    votes = 0\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "    for ind in data.index:\n",
    "            filename = df['Patch_Filename'][ind]\n",
    "            pred_patch_class = df['Predicted Class'][ind]\n",
    "            pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "            \n",
    "            filename = filename.split(\"_\")\n",
    "            file_length = len(filename)\n",
    "            initial_filename = filename[:-1]\n",
    "            patch_name = filename[file_length-1]\n",
    "            #print(initial_filename)\n",
    "            #print(patch_name)\n",
    "\n",
    "            if(a!= initial_filename and a==\"a\"):\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a!=initial_filename and a!=\"a\"):\n",
    "                total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                counts = np.bincount(arr_pred_patches)\n",
    "                pred_image_class = np.argmax(counts)\n",
    "                votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                s=0\n",
    "                \n",
    "                if(pred_image_class == true_image_class):\n",
    "                    correct_images_perclass = correct_images_perclass + 1\n",
    "                    total_class_votes = total_class_votes + votes\n",
    "                    total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                    \n",
    "                    total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                    z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                    for i in range(0,len(arr_pred_patches)):\n",
    "                        if(z[i]==1):\n",
    "                            s= s+1\n",
    "                            prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "            \n",
    "                arr_pred_patches = []\n",
    "                arr_pred_patches_prob = []\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a==initial_filename):\n",
    "                total_patches_perclass = total_patches_perclass + 1\n",
    "                arr_pred_patches.append(pred_patch_class)\n",
    "                arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "    total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "    counts = np.bincount(arr_pred_patches)\n",
    "    pred_image_class = np.argmax(counts)\n",
    "    votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "    if(pred_image_class == true_image_class):\n",
    "        correct_images_perclass = correct_images_perclass + 1\n",
    "        total_class_votes = total_class_votes + votes\n",
    "        total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "        \n",
    "        total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "        z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "        for i in range(0,len(arr_pred_patches)):\n",
    "            if(z[i]==1):\n",
    "                prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    \n",
    "    if(correct_images_perclass!=0):\n",
    "        prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "        prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "        avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "        print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "    else:\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "            \n",
    "\n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'))\n",
    "\n",
    "ILA_random = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_random}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8cb9981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_obj_36_79',) 0 0 1.0\n",
      "1000 ('D01_tex_29_58',) 0 0 1.0\n",
      "2000 ('D01_nat_45_9',) 0 0 0.84\n",
      "3000 ('D01_obj_2_7',) 0 0 0.96\n",
      "4000 ('D01_nat_29_4',) 0 0 0.92\n",
      "Yes_Class 28\n",
      "5000 ('D29_nat_1_62',) 28 28 1.0\n",
      "6000 ('D29_nat_30_58',) 28 28 0.67\n",
      "7000 ('D29_nat_9_75',) 28 28 0.99\n",
      "8000 ('D29_nat_2_121',) 28 28 0.98\n",
      "Yes_Class 22\n",
      "9000 ('D23_obj_1_37',) 22 22 1.0\n",
      "10000 ('D23_obj_30_11',) 22 22 1.0\n",
      "11000 ('D23_tex_2_112',) 22 22 0.63\n",
      "12000 ('D23_obj_29_126',) 22 22 0.96\n",
      "Yes_Class 27\n",
      "13000 ('D28_nat_25_124',) 27 19 0.94\n",
      "14000 ('D28_nat_45_0',) 27 27 0.7\n",
      "15000 ('D28_nat_30_56',) 27 27 0.38\n",
      "16000 ('D28_tex_39_17',) 27 27 0.71\n",
      "Yes_Class 3\n",
      "17000 ('D04_tex_7_105',) 3 3 1.0\n",
      "18000 ('D04_obj_5_113',) 3 3 1.0\n",
      "19000 ('D04_color_2_61',) 3 3 1.0\n",
      "20000 ('D04_nat_36_100',) 3 3 1.0\n",
      "Yes_Class 14\n",
      "21000 ('D15_nat_2_60',) 14 14 0.91\n",
      "22000 ('D15_nat_49_90',) 14 14 0.64\n",
      "23000 ('D15_nat_36_48',) 14 14 1.0\n",
      "24000 ('D15_obj_29_80',) 14 14 1.0\n",
      "Yes_Class 20\n",
      "25000 ('D21_nat_49_96',) 20 20 1.0\n",
      "26000 ('D21_tex_25_99',) 20 20 1.0\n",
      "27000 ('D21_tex_39_14',) 20 20 0.99\n",
      "28000 ('D21_tex_7_91',) 20 20 1.0\n",
      "Yes_Class 4\n",
      "29000 ('D05_nat_30_118',) 4 4 0.99\n",
      "30000 ('D05_tex_7_88',) 4 4 0.93\n",
      "31000 ('D05_nat_29_125',) 4 4 0.99\n",
      "32000 ('D05_tex_7_97',) 4 4 0.95\n",
      "Yes_Class 18\n",
      "33000 ('D19_obj_17_89',) 18 18 1.0\n",
      "34000 ('D19_nat_17_105',) 18 18 1.0\n",
      "35000 ('D19_tex_7_50',) 18 18 0.99\n",
      "36000 ('D19_nat_17_59',) 18 17 0.74\n",
      "Yes_Class 17\n",
      "37000 ('D18_tex_1_50',) 17 24 0.57\n",
      "38000 ('D18_nat_30_75',) 17 17 0.98\n",
      "39000 ('D18_color_14_75',) 17 1 0.86\n",
      "40000 ('D18_nat_9_19',) 17 17 0.85\n",
      "Yes_Class 10\n",
      "41000 ('D11_obj_36_96',) 10 10 1.0\n",
      "42000 ('D11_nat_49_121',) 10 10 1.0\n",
      "43000 ('D11_obj_25_16',) 10 10 0.99\n",
      "44000 ('D11_obj_1_73',) 10 10 0.96\n",
      "45000 ('D11_tex_17_126',) 10 10 1.0\n",
      "Yes_Class 12\n",
      "46000 ('D13_nat_2_99',) 12 12 0.95\n",
      "47000 ('D13_obj_30_114',) 12 12 0.99\n",
      "48000 ('D13_color_2_20',) 12 11 0.53\n",
      "49000 ('D13_color_2_118',) 12 12 0.62\n",
      "Yes_Class 19\n",
      "50000 ('D20_obj_29_68',) 19 19 1.0\n",
      "51000 ('D20_nat_36_115',) 19 13 0.79\n",
      "52000 ('D20_tex_7_10',) 19 19 1.0\n",
      "53000 ('D20_color_14_73',) 19 19 0.99\n",
      "Yes_Class 5\n",
      "54000 ('D06_nat_36_36',) 5 28 0.39\n",
      "55000 ('D06_nat_36_63',) 5 27 0.6\n",
      "56000 ('D06_nat_25_28',) 5 5 0.44\n",
      "57000 ('D06_nat_49_102',) 5 6 0.37\n",
      "Yes_Class 1\n",
      "58000 ('D02_tex_17_104',) 1 1 1.0\n",
      "59000 ('D02_obj_25_25',) 1 1 0.9\n",
      "60000 ('D02_obj_30_126',) 1 1 1.0\n",
      "61000 ('D02_tex_1_104',) 1 13 0.73\n",
      "Yes_Class 2\n",
      "62000 ('D03_nat_9_105',) 2 2 1.0\n",
      "63000 ('D03_obj_29_34',) 2 2 0.94\n",
      "64000 ('D03_tex_39_75',) 2 2 1.0\n",
      "65000 ('D03_nat_25_109',) 2 2 0.46\n",
      "Yes_Class 21\n",
      "66000 ('D22_obj_30_127',) 21 21 1.0\n",
      "67000 ('D22_tex_36_89',) 21 21 0.67\n",
      "68000 ('D22_color_14_26',) 21 8 0.95\n",
      "69000 ('D22_obj_17_50',) 21 21 1.0\n",
      "Yes_Class 23\n",
      "70000 ('D24_nat_30_67',) 23 29 0.91\n",
      "71000 ('D24_nat_9_75',) 23 23 0.55\n",
      "72000 ('D24_tex_17_85',) 23 23 1.0\n",
      "73000 ('D24_obj_30_96',) 23 23 1.0\n",
      "Yes_Class 25\n",
      "74000 ('D26_tex_39_88',) 25 25 1.0\n",
      "75000 ('D26_tex_30_51',) 25 25 1.0\n",
      "76000 ('D26_obj_36_30',) 25 25 0.97\n",
      "77000 ('D26_nat_17_36',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "78000 ('D10_color_4_79',) 9 9 1.0\n",
      "79000 ('D10_tex_25_39',) 9 9 1.0\n",
      "80000 ('D10_tex_17_65',) 9 9 1.0\n",
      "81000 ('D10_obj_2_30',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "82000 ('D30_obj_29_125',) 29 29 1.0\n",
      "83000 ('D30_obj_5_75',) 29 29 1.0\n",
      "84000 ('D30_tex_2_9',) 29 29 1.0\n",
      "85000 ('D30_obj_17_112',) 29 29 1.0\n",
      "86000 ('D30_color_4_77',) 29 29 1.0\n",
      "Yes_Class 7\n",
      "87000 ('D08_obj_29_18',) 7 7 1.0\n",
      "88000 ('D08_tex_36_75',) 7 7 1.0\n",
      "89000 ('D08_tex_36_71',) 7 7 1.0\n",
      "90000 ('D08_tex_25_28',) 7 7 0.98\n",
      "Yes_Class 13\n",
      "91000 ('D14_tex_36_125',) 13 13 0.86\n",
      "92000 ('D14_obj_30_72',) 13 13 0.8\n",
      "93000 ('D14_tex_25_58',) 13 13 0.85\n",
      "94000 ('D14_tex_39_110',) 13 13 0.8\n",
      "Yes_Class 6\n",
      "95000 ('D07_color_4_93',) 6 6 0.76\n",
      "96000 ('D07_tex_39_97',) 6 6 1.0\n",
      "97000 ('D07_obj_5_6',) 6 6 1.0\n",
      "98000 ('D07_nat_45_121',) 6 6 1.0\n",
      "Yes_Class 16\n",
      "99000 ('D17_obj_1_66',) 16 16 1.0\n",
      "100000 ('D17_nat_25_92',) 16 16 0.93\n",
      "101000 ('D17_nat_36_64',) 16 5 0.8\n",
      "102000 ('D17_tex_25_73',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "103000 ('D27_nat_39_110',) 26 26 1.0\n",
      "104000 ('D27_tex_7_75',) 26 23 0.69\n",
      "105000 ('D27_tex_7_40',) 26 26 0.78\n",
      "106000 ('D27_tex_2_93',) 26 26 0.99\n",
      "Yes_Class 11\n",
      "107000 ('D12_tex_36_63',) 11 11 0.78\n",
      "108000 ('D12_nat_39_121',) 11 11 1.0\n",
      "109000 ('D12_nat_29_59',) 11 11 0.74\n",
      "110000 ('D12_color_14_100',) 11 11 0.87\n",
      "Yes_Class 15\n",
      "111000 ('D16_nat_9_74',) 15 13 0.9\n",
      "112000 ('D16_tex_7_106',) 15 15 0.78\n",
      "113000 ('D16_obj_5_19',) 15 15 1.0\n",
      "114000 ('D16_obj_1_91',) 15 15 0.91\n",
      "Yes_Class 24\n",
      "115000 ('D25_nat_29_35',) 24 17 0.46\n",
      "116000 ('D25_nat_36_57',) 24 24 0.77\n",
      "117000 ('D25_obj_17_70',) 24 24 0.94\n",
      "118000 ('D25_tex_30_97',) 24 24 0.92\n",
      "Yes_Class 8\n",
      "119000 ('D09_nat_39_59',) 8 8 1.0\n",
      "120000 ('D09_obj_30_26',) 8 8 0.98\n",
      "121000 ('D09_nat_25_8',) 8 8 0.95\n",
      "122000 ('D09_color_2_83',) 8 8 1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Similar__\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Similar__ folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a=-1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1, labels1,patch_filename) in enumerate(test_loader_similar):\n",
    "        _, c = torch.max(labels1.data,1)\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org,target = imgs1.to(device,dtype=torch.float), labels1.to(device)\n",
    "        #img_org = img_org.permute(0, 3, 1, 2)\n",
    "        \n",
    "        output = model(img_org)\n",
    "        \n",
    "\n",
    "        _, actual = torch.max(target.data, 1)    \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87cf4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Similar__/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results_Random.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c777f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 4096 29 3001 80.85 0.87\n",
      "32 4096 32 3806 92.92 0.94\n",
      "32 4096 32 3881 94.75 0.97\n",
      "32 4096 32 3619 88.35 0.91\n",
      "32 4096 32 3863 94.31 0.97\n",
      "32 4096 29 3397 91.51 0.89\n",
      "32 4096 32 4079 99.58 0.99\n",
      "32 4096 32 3955 96.56 0.98\n",
      "32 4096 30 3374 87.86 0.89\n",
      "32 4096 32 3800 92.77 0.95\n",
      "32 4096 32 3533 86.25 0.88\n",
      "32 4096 32 3714 90.67 0.94\n",
      "32 4096 32 3587 87.57 0.91\n",
      "32 4096 32 3891 95.0 0.96\n",
      "32 4096 32 3942 96.24 0.97\n",
      "32 4096 31 3529 88.94 0.84\n",
      "32 4096 30 3285 85.55 0.81\n",
      "32 4096 28 2883 80.44 0.91\n",
      "32 4096 28 3171 88.48 0.93\n",
      "32 4096 31 3801 95.79 0.96\n",
      "32 4096 32 4044 98.73 0.99\n",
      "32 4096 27 3102 89.76 0.86\n",
      "32 4096 24 2164 70.44 0.76\n",
      "32 4096 32 4058 99.07 0.99\n",
      "32 4096 27 2575 74.51 0.72\n",
      "32 4096 32 3414 83.35 0.92\n",
      "32 4096 31 3693 93.07 0.96\n",
      "32 4096 32 3954 96.53 0.97\n",
      "32 4096 30 3654 95.16 0.95\n",
      "32 4096 32 3930 95.95 0.95\n",
      "Image Level Accuracy: 95.72916666666667%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    data = df.sort_values(by=['Patch_Filename'])\n",
    "    classname = Path(f).stem\n",
    "    classname = int(classname.split(\"_\")[2])\n",
    "    a=\"a\"\n",
    "\n",
    "    true_image_class = classname\n",
    "    total_images_perclass = 0\n",
    "    correct_images_perclass = 0\n",
    "    total_patches_perclass = 0\n",
    "\n",
    "    total_class_votes =0\n",
    "    total_class_patches =0\n",
    "    prob_avg_correct_patch = 0.0\n",
    "    votes = 0\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "    for ind in data.index:\n",
    "            filename = df['Patch_Filename'][ind]\n",
    "            pred_patch_class = df['Predicted Class'][ind]\n",
    "            pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "            \n",
    "            filename = filename.split(\"_\")\n",
    "            file_length = len(filename)\n",
    "            initial_filename = filename[:-1]\n",
    "            patch_name = filename[file_length-1]\n",
    "            #print(initial_filename)\n",
    "            #print(patch_name)\n",
    "\n",
    "            if(a!= initial_filename and a==\"a\"):\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a!=initial_filename and a!=\"a\"):\n",
    "                total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                counts = np.bincount(arr_pred_patches)\n",
    "                pred_image_class = np.argmax(counts)\n",
    "                votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                s=0\n",
    "                \n",
    "                if(pred_image_class == true_image_class):\n",
    "                    correct_images_perclass = correct_images_perclass + 1\n",
    "                    total_class_votes = total_class_votes + votes\n",
    "                    total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                    \n",
    "                    total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                    z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                    for i in range(0,len(arr_pred_patches)):\n",
    "                        if(z[i]==1):\n",
    "                            s= s+1\n",
    "                            prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "            \n",
    "                arr_pred_patches = []\n",
    "                arr_pred_patches_prob = []\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a==initial_filename):\n",
    "                total_patches_perclass = total_patches_perclass + 1\n",
    "                arr_pred_patches.append(pred_patch_class)\n",
    "                arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "    total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "    counts = np.bincount(arr_pred_patches)\n",
    "    pred_image_class = np.argmax(counts)\n",
    "    votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "    if(pred_image_class == true_image_class):\n",
    "        correct_images_perclass = correct_images_perclass + 1\n",
    "        total_class_votes = total_class_votes + votes\n",
    "        total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "        \n",
    "        total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "        z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "        for i in range(0,len(arr_pred_patches)):\n",
    "            if(z[i]==1):\n",
    "                prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    \n",
    "    if(correct_images_perclass!=0):\n",
    "        prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "        prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "        avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "        print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "    else:\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "            \n",
    "\n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'))\n",
    "\n",
    "ILA_similar = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_similar}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea7a9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_rnd_34_15',) 0 0 0.99\n",
      "1000 ('D01_rnd_62_65',) 0 0 1.0\n",
      "2000 ('D01_obj_25_86',) 0 0 0.9\n",
      "3000 ('D01_rnd_141_66',) 0 13 0.6\n",
      "4000 ('D01_rnd_86_8',) 0 0 1.0\n",
      "5000 ('D01_tex_39_45',) 0 0 1.0\n",
      "6000 ('D01_nat_2_29',) 0 0 0.66\n",
      "7000 ('D01_rnd_35_66',) 0 0 0.84\n",
      "8000 ('D01_tex_25_49',) 0 0 0.96\n",
      "Yes_Class 28\n",
      "9000 ('D29_rnd_2_31',) 28 28 1.0\n",
      "10000 ('D29_rnd_30_83',) 28 5 0.39\n",
      "11000 ('D29_obj_1_126',) 28 28 1.0\n",
      "12000 ('D29_rnd_59_47',) 28 28 1.0\n",
      "13000 ('D29_rnd_7_78',) 28 28 1.0\n",
      "14000 ('D29_rnd_160_102',) 28 28 0.99\n",
      "15000 ('D29_nat_39_81',) 28 28 1.0\n",
      "16000 ('D29_rnd_141_88',) 28 28 0.37\n",
      "17000 ('D29_rnd_129_79',) 28 28 1.0\n",
      "Yes_Class 22\n",
      "18000 ('D23_obj_29_89',) 22 22 0.83\n",
      "19000 ('D23_rnd_70_60',) 22 10 0.49\n",
      "20000 ('D23_rnd_55_39',) 22 22 0.84\n",
      "21000 ('D23_rnd_43_114',) 22 22 0.99\n",
      "22000 ('D23_nat_9_57',) 22 22 1.0\n",
      "23000 ('D23_tex_17_41',) 22 22 0.99\n",
      "24000 ('D23_tex_30_41',) 22 22 1.0\n",
      "Yes_Class 27\n",
      "25000 ('D28_color_4_50',) 27 27 0.88\n",
      "26000 ('D28_rnd_67_107',) 27 6 0.48\n",
      "27000 ('D28_obj_25_84',) 27 27 0.55\n",
      "28000 ('D28_rnd_117_89',) 27 27 0.43\n",
      "29000 ('D28_nat_30_42',) 27 27 0.91\n",
      "30000 ('D28_obj_2_90',) 27 27 0.6\n",
      "31000 ('D28_nat_30_30',) 27 27 0.59\n",
      "32000 ('D28_obj_39_55',) 27 27 0.83\n",
      "33000 ('D28_rnd_132_43',) 27 4 0.22\n",
      "Yes_Class 3\n",
      "34000 ('D04_obj_29_43',) 3 3 1.0\n",
      "35000 ('D04_rnd_72_102',) 3 3 1.0\n",
      "36000 ('D04_rnd_214_42',) 3 3 1.0\n",
      "37000 ('D04_nat_30_24',) 3 3 1.0\n",
      "38000 ('D04_nat_39_41',) 3 3 1.0\n",
      "39000 ('D04_rnd_240_42',) 3 3 1.0\n",
      "40000 ('D04_obj_39_36',) 3 3 1.0\n",
      "41000 ('D04_tex_29_47',) 3 3 0.83\n",
      "42000 ('D04_rnd_153_14',) 3 3 1.0\n",
      "43000 ('D04_obj_5_80',) 3 3 1.0\n",
      "Yes_Class 14\n",
      "44000 ('D15_rnd_119_103',) 14 14 1.0\n",
      "45000 ('D15_rnd_274_13',) 14 14 1.0\n",
      "46000 ('D15_rnd_190_123',) 14 14 0.99\n",
      "47000 ('D15_obj_36_91',) 14 14 0.36\n",
      "48000 ('D15_nat_17_92',) 14 14 0.99\n",
      "49000 ('D15_nat_25_96',) 14 14 0.98\n",
      "50000 ('D15_rnd_109_115',) 14 14 0.97\n",
      "51000 ('D15_nat_36_13',) 14 14 1.0\n",
      "52000 ('D15_rnd_274_116',) 14 14 0.99\n",
      "53000 ('D15_rnd_262_124',) 14 14 0.95\n",
      "54000 ('D15_rnd_146_49',) 14 10 0.81\n",
      "Yes_Class 20\n",
      "55000 ('D21_nat_30_19',) 20 4 0.51\n",
      "56000 ('D21_rnd_129_101',) 20 4 0.86\n",
      "57000 ('D21_rnd_99_43',) 20 13 0.44\n",
      "58000 ('D21_rnd_122_83',) 20 20 0.97\n",
      "59000 ('D21_rnd_67_123',) 20 20 0.94\n",
      "60000 ('D21_rnd_67_104',) 20 20 0.94\n",
      "61000 ('D21_rnd_51_82',) 20 20 0.53\n",
      "62000 ('D21_rnd_65_87',) 20 20 0.99\n",
      "Yes_Class 4\n",
      "63000 ('D05_obj_36_127',) 4 4 0.98\n",
      "64000 ('D05_tex_7_48',) 4 4 0.98\n",
      "65000 ('D05_tex_36_110',) 4 4 0.99\n",
      "66000 ('D05_rnd_64_10',) 4 4 0.86\n",
      "67000 ('D05_tex_7_101',) 4 4 0.73\n",
      "68000 ('D05_obj_2_84',) 4 4 1.0\n",
      "69000 ('D05_color_2_69',) 4 1 0.53\n",
      "70000 ('D05_rnd_153_72',) 4 4 0.98\n",
      "71000 ('D05_rnd_113_114',) 4 4 0.64\n",
      "Yes_Class 18\n",
      "72000 ('D19_nat_25_97',) 18 18 1.0\n",
      "73000 ('D19_obj_39_125',) 18 18 1.0\n",
      "74000 ('D19_rnd_129_24',) 18 18 1.0\n",
      "75000 ('D19_rnd_196_60',) 18 18 1.0\n",
      "76000 ('D19_tex_2_46',) 18 18 1.0\n",
      "77000 ('D19_rnd_188_44',) 18 18 1.0\n",
      "78000 ('D19_rnd_122_77',) 18 18 0.75\n",
      "79000 ('D19_rnd_70_51',) 18 18 1.0\n",
      "80000 ('D19_rnd_150_85',) 18 18 1.0\n",
      "81000 ('D19_nat_39_76',) 18 18 1.0\n",
      "Yes_Class 17\n",
      "82000 ('D18_rnd_160_32',) 17 17 1.0\n",
      "83000 ('D18_obj_5_96',) 17 17 0.95\n",
      "84000 ('D18_rnd_150_98',) 17 17 0.53\n",
      "85000 ('D18_rnd_6_71',) 17 17 0.75\n",
      "86000 ('D18_rnd_119_7',) 17 17 0.74\n",
      "87000 ('D18_color_2_97',) 17 17 0.92\n",
      "88000 ('D18_obj_29_38',) 17 17 0.95\n",
      "89000 ('D18_obj_36_109',) 17 17 0.97\n",
      "Yes_Class 10\n",
      "90000 ('D11_obj_29_45',) 10 10 1.0\n",
      "91000 ('D11_rnd_166_4',) 10 10 1.0\n",
      "92000 ('D11_nat_49_39',) 10 0 0.45\n",
      "93000 ('D11_rnd_99_79',) 10 10 0.97\n",
      "94000 ('D11_nat_36_104',) 10 10 0.99\n",
      "95000 ('D11_nat_36_58',) 10 10 1.0\n",
      "96000 ('D11_nat_17_67',) 10 10 0.78\n",
      "97000 ('D11_nat_17_100',) 10 10 0.46\n",
      "Yes_Class 12\n",
      "98000 ('D13_rnd_108_40',) 12 12 0.49\n",
      "99000 ('D13_nat_9_100',) 12 12 0.95\n",
      "100000 ('D13_nat_39_61',) 12 12 0.63\n",
      "101000 ('D13_rnd_153_122',) 12 12 0.81\n",
      "102000 ('D13_rnd_130_66',) 12 12 0.99\n",
      "103000 ('D13_nat_9_68',) 12 12 0.74\n",
      "104000 ('D13_rnd_63_123',) 12 12 0.82\n",
      "105000 ('D13_rnd_117_58',) 12 12 0.4\n",
      "106000 ('D13_rnd_44_64',) 12 12 0.93\n",
      "Yes_Class 19\n",
      "107000 ('D20_nat_30_118',) 19 19 1.0\n",
      "108000 ('D20_nat_17_101',) 19 19 0.98\n",
      "109000 ('D20_rnd_138_117',) 19 19 0.92\n",
      "110000 ('D20_tex_29_103',) 19 19 0.99\n",
      "111000 ('D20_rnd_32_83',) 19 19 0.99\n",
      "112000 ('D20_nat_25_37',) 19 19 0.9\n",
      "113000 ('D20_nat_45_53',) 19 19 1.0\n",
      "114000 ('D20_tex_25_39',) 19 19 1.0\n",
      "Yes_Class 5\n",
      "115000 ('D06_nat_1_91',) 5 5 0.91\n",
      "116000 ('D06_nat_36_51',) 5 28 0.48\n",
      "117000 ('D06_tex_39_36',) 5 5 0.74\n",
      "118000 ('D06_nat_39_15',) 5 6 0.64\n",
      "Yes_Class 1\n",
      "119000 ('D02_rnd_39_85',) 1 1 0.8\n",
      "120000 ('D02_rnd_3_35',) 1 1 0.88\n",
      "121000 ('D02_obj_25_115',) 1 1 0.81\n",
      "122000 ('D02_nat_45_102',) 1 1 0.98\n",
      "123000 ('D02_rnd_96_99',) 1 1 0.94\n",
      "124000 ('D02_rnd_44_38',) 1 1 1.0\n",
      "125000 ('D02_nat_9_69',) 1 1 1.0\n",
      "126000 ('D02_rnd_202_101',) 1 1 0.94\n",
      "127000 ('D02_tex_2_103',) 1 1 0.96\n",
      "128000 ('D02_rnd_64_39',) 1 0 0.86\n",
      "Yes_Class 2\n",
      "129000 ('D03_rnd_52_34',) 2 2 1.0\n",
      "130000 ('D03_rnd_130_92',) 2 2 1.0\n",
      "131000 ('D03_nat_2_92',) 2 2 0.99\n",
      "132000 ('D03_rnd_141_98',) 2 2 1.0\n",
      "133000 ('D03_rnd_141_60',) 2 2 1.0\n",
      "134000 ('D03_rnd_130_122',) 2 2 1.0\n",
      "135000 ('D03_nat_49_33',) 2 2 1.0\n",
      "136000 ('D03_nat_36_6',) 2 2 1.0\n",
      "137000 ('D03_rnd_46_36',) 2 2 1.0\n",
      "138000 ('D03_nat_36_67',) 2 2 1.0\n",
      "Yes_Class 21\n",
      "139000 ('D22_rnd_130_66',) 21 21 0.92\n",
      "140000 ('D22_nat_39_54',) 21 21 0.97\n",
      "141000 ('D22_nat_29_95',) 21 21 0.98\n",
      "142000 ('D22_rnd_130_89',) 21 21 0.72\n",
      "143000 ('D22_rnd_39_2',) 21 21 0.33\n",
      "144000 ('D22_rnd_129_124',) 21 21 1.0\n",
      "145000 ('D22_rnd_69_110',) 21 21 0.98\n",
      "146000 ('D22_nat_2_52',) 21 21 0.92\n",
      "Yes_Class 23\n",
      "147000 ('D24_rnd_54_123',) 23 23 0.98\n",
      "148000 ('D24_rnd_122_102',) 23 23 0.43\n",
      "149000 ('D24_obj_29_123',) 23 23 1.0\n",
      "150000 ('D24_rnd_10_83',) 23 23 1.0\n",
      "151000 ('D24_tex_36_36',) 23 23 1.0\n",
      "152000 ('D24_tex_2_111',) 23 23 0.69\n",
      "153000 ('D24_obj_36_60',) 23 23 0.98\n",
      "154000 ('D24_nat_30_12',) 23 23 1.0\n",
      "Yes_Class 25\n",
      "155000 ('D26_nat_9_73',) 25 25 1.0\n",
      "156000 ('D26_rnd_130_61',) 25 25 1.0\n",
      "157000 ('D26_obj_29_66',) 25 25 0.98\n",
      "158000 ('D26_rnd_150_45',) 25 25 1.0\n",
      "159000 ('D26_rnd_68_86',) 25 25 1.0\n",
      "160000 ('D26_nat_39_100',) 25 25 1.0\n",
      "161000 ('D26_nat_49_29',) 25 25 0.97\n",
      "162000 ('D26_rnd_29_10',) 25 25 0.99\n",
      "163000 ('D26_tex_29_78',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "164000 ('D10_rnd_150_122',) 9 9 1.0\n",
      "165000 ('D10_rnd_10_96',) 9 8 0.73\n",
      "166000 ('D10_nat_45_30',) 9 9 1.0\n",
      "167000 ('D10_tex_29_24',) 9 9 1.0\n",
      "168000 ('D10_rnd_41_86',) 9 9 1.0\n",
      "169000 ('D10_rnd_129_88',) 9 9 1.0\n",
      "170000 ('D10_nat_39_89',) 9 9 1.0\n",
      "171000 ('D10_obj_30_2',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "172000 ('D30_rnd_122_1',) 29 29 1.0\n",
      "173000 ('D30_nat_29_108',) 29 29 1.0\n",
      "174000 ('D30_rnd_150_72',) 29 29 1.0\n",
      "175000 ('D30_obj_1_104',) 29 29 1.0\n",
      "176000 ('D30_rnd_108_99',) 29 29 1.0\n",
      "177000 ('D30_rnd_141_43',) 29 29 1.0\n",
      "178000 ('D30_rnd_110_47',) 29 29 1.0\n",
      "179000 ('D30_obj_30_107',) 29 29 1.0\n",
      "Yes_Class 7\n",
      "180000 ('D08_rnd_63_91',) 7 7 0.98\n",
      "181000 ('D08_tex_1_84',) 7 7 0.97\n",
      "182000 ('D08_rnd_129_116',) 7 7 1.0\n",
      "183000 ('D08_rnd_185_62',) 7 7 0.98\n",
      "184000 ('D08_nat_49_18',) 7 7 1.0\n",
      "185000 ('D08_obj_5_49',) 7 7 0.99\n",
      "186000 ('D08_rnd_135_81',) 7 7 1.0\n",
      "187000 ('D08_rnd_160_97',) 7 7 1.0\n",
      "188000 ('D08_obj_2_29',) 7 7 1.0\n",
      "Yes_Class 13\n",
      "189000 ('D14_rnd_135_104',) 13 0 0.54\n",
      "190000 ('D14_tex_2_17',) 13 13 0.97\n",
      "191000 ('D14_nat_29_77',) 13 13 0.95\n",
      "192000 ('D14_tex_2_25',) 13 13 0.96\n",
      "193000 ('D14_obj_30_63',) 13 13 0.96\n",
      "194000 ('D14_rnd_150_102',) 13 13 0.79\n",
      "195000 ('D14_obj_17_93',) 13 13 0.57\n",
      "196000 ('D14_tex_30_18',) 13 13 0.92\n",
      "197000 ('D14_rnd_3_109',) 13 28 0.86\n",
      "Yes_Class 6\n",
      "198000 ('D07_rnd_35_24',) 6 6 1.0\n",
      "199000 ('D07_rnd_122_78',) 6 15 0.25\n",
      "200000 ('D07_tex_2_51',) 6 6 1.0\n",
      "201000 ('D07_rnd_124_95',) 6 6 0.89\n",
      "202000 ('D07_rnd_55_77',) 6 6 0.74\n",
      "203000 ('D07_tex_1_58',) 6 6 0.98\n",
      "204000 ('D07_obj_30_68',) 6 6 0.4\n",
      "205000 ('D07_tex_39_80',) 6 6 0.98\n",
      "206000 ('D07_nat_9_70',) 6 27 0.47\n",
      "Yes_Class 16\n",
      "207000 ('D17_rnd_129_31',) 16 16 1.0\n",
      "208000 ('D17_rnd_87_106',) 16 16 1.0\n",
      "209000 ('D17_rnd_117_69',) 16 16 1.0\n",
      "210000 ('D17_tex_29_27',) 16 16 1.0\n",
      "211000 ('D17_rnd_5_101',) 16 16 0.97\n",
      "212000 ('D17_rnd_78_24',) 16 16 1.0\n",
      "213000 ('D17_rnd_64_115',) 16 16 1.0\n",
      "214000 ('D17_nat_49_103',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "215000 ('D27_rnd_130_62',) 26 26 1.0\n",
      "216000 ('D27_rnd_135_14',) 26 26 1.0\n",
      "217000 ('D27_rnd_141_37',) 26 26 1.0\n",
      "218000 ('D27_tex_39_9',) 26 26 0.82\n",
      "219000 ('D27_color_4_4',) 26 26 0.65\n",
      "220000 ('D27_obj_1_41',) 26 26 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221000 ('D27_tex_17_48',) 26 26 1.0\n",
      "222000 ('D27_obj_25_104',) 26 26 0.88\n",
      "Yes_Class 11\n",
      "223000 ('D12_nat_45_98',) 11 11 0.96\n",
      "224000 ('D12_obj_2_105',) 11 11 1.0\n",
      "225000 ('D12_rnd_130_113',) 11 11 0.99\n",
      "226000 ('D12_rnd_119_99',) 11 11 0.75\n",
      "227000 ('D12_tex_30_123',) 11 11 0.63\n",
      "228000 ('D12_tex_30_113',) 11 11 0.63\n",
      "229000 ('D12_tex_25_114',) 11 11 1.0\n",
      "230000 ('D12_rnd_150_82',) 11 11 0.95\n",
      "231000 ('D12_rnd_71_96',) 11 11 0.98\n",
      "Yes_Class 15\n",
      "232000 ('D16_color_4_43',) 15 15 0.92\n",
      "233000 ('D16_nat_49_9',) 15 0 0.49\n",
      "234000 ('D16_rnd_73_60',) 15 15 0.98\n",
      "235000 ('D16_nat_25_56',) 15 15 0.66\n",
      "236000 ('D16_nat_29_115',) 15 15 0.51\n",
      "237000 ('D16_rnd_110_76',) 15 15 1.0\n",
      "238000 ('D16_color_4_97',) 15 15 1.0\n",
      "239000 ('D16_rnd_35_108',) 15 15 0.88\n",
      "Yes_Class 24\n",
      "240000 ('D25_rnd_80_113',) 24 24 0.63\n",
      "241000 ('D25_rnd_47_85',) 24 24 0.98\n",
      "242000 ('D25_rnd_117_60',) 24 24 0.92\n",
      "243000 ('D25_rnd_5_119',) 24 17 0.51\n",
      "244000 ('D25_rnd_63_19',) 24 24 0.61\n",
      "245000 ('D25_rnd_117_2',) 24 24 0.81\n",
      "246000 ('D25_nat_29_90',) 24 24 0.6\n",
      "247000 ('D25_nat_36_99',) 24 24 0.98\n",
      "Yes_Class 8\n",
      "248000 ('D09_rnd_150_94',) 8 8 1.0\n",
      "249000 ('D09_rnd_138_53',) 8 8 0.84\n",
      "250000 ('D09_rnd_35_11',) 8 8 1.0\n",
      "251000 ('D09_nat_39_20',) 8 8 1.0\n",
      "252000 ('D09_rnd_54_75',) 8 8 0.8\n",
      "253000 ('D09_rnd_36_12',) 8 8 1.0\n",
      "254000 ('D09_rnd_63_78',) 8 8 0.82\n",
      "255000 ('D09_rnd_150_59',) 8 8 0.79\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Merged__\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Merged__ folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a=-1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1, labels1,patch_filename) in enumerate(test_loader_merged):\n",
    "        _, c = torch.max(labels1.data,1)\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org,target = imgs1.to(device,dtype=torch.float), labels1.to(device)\n",
    "        #img_org = img_org.permute(0, 3, 1, 2)\n",
    "        \n",
    "        output = model(img_org)\n",
    "        \n",
    "\n",
    "        _, actual = torch.max(target.data, 1)    \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec881ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Merged__/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results_Random.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c51ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 8704 62 6364 80.19 0.86\n",
      "64 8192 64 7545 92.1 0.93\n",
      "75 9600 74 8961 94.61 0.97\n",
      "70 8960 66 7639 90.42 0.9\n",
      "64 8192 63 7431 92.15 0.96\n",
      "66 8448 63 7287 90.36 0.87\n",
      "64 8192 62 7852 98.94 0.99\n",
      "72 9216 70 8734 97.48 0.98\n",
      "76 9728 73 8280 88.61 0.9\n",
      "79 10112 78 9263 92.78 0.96\n",
      "67 8576 61 6468 82.84 0.87\n",
      "63 8064 63 7276 90.23 0.93\n",
      "66 8448 65 7266 87.33 0.91\n",
      "66 8448 64 7497 91.52 0.94\n",
      "65 8320 63 7714 95.66 0.97\n",
      "63 8064 61 6631 84.93 0.82\n",
      "66 8448 62 6619 83.4 0.82\n",
      "63 8064 54 5106 73.87 0.87\n",
      "63 8064 58 6812 91.76 0.96\n",
      "88 11264 80 9460 92.38 0.94\n",
      "83 10624 81 10225 98.62 0.99\n",
      "65 8320 59 6633 87.83 0.88\n",
      "32 4096 24 2164 70.44 0.76\n",
      "65 8320 65 8176 98.27 0.99\n",
      "63 8064 45 4327 75.12 0.74\n",
      "62 7936 61 6683 85.59 0.92\n",
      "64 8192 63 7231 89.67 0.95\n",
      "64 8192 64 7967 97.25 0.97\n",
      "67 8576 64 7751 94.62 0.94\n",
      "67 8576 65 7747 93.11 0.94\n",
      "Image Level Accuracy: 94.85%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    data = df.sort_values(by=['Patch_Filename'])\n",
    "    classname = Path(f).stem\n",
    "    classname = int(classname.split(\"_\")[2])\n",
    "    a=\"a\"\n",
    "\n",
    "    true_image_class = classname\n",
    "    total_images_perclass = 0\n",
    "    correct_images_perclass = 0\n",
    "    total_patches_perclass = 0\n",
    "\n",
    "    total_class_votes =0\n",
    "    total_class_patches =0\n",
    "    prob_avg_correct_patch = 0.0\n",
    "    votes = 0\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "    for ind in data.index:\n",
    "            filename = df['Patch_Filename'][ind]\n",
    "            pred_patch_class = df['Predicted Class'][ind]\n",
    "            pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "            \n",
    "            filename = filename.split(\"_\")\n",
    "            file_length = len(filename)\n",
    "            initial_filename = filename[:-1]\n",
    "            patch_name = filename[file_length-1]\n",
    "            #print(initial_filename)\n",
    "            #print(patch_name)\n",
    "\n",
    "            if(a!= initial_filename and a==\"a\"):\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a!=initial_filename and a!=\"a\"):\n",
    "                total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                counts = np.bincount(arr_pred_patches)\n",
    "                pred_image_class = np.argmax(counts)\n",
    "                votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                s=0\n",
    "                \n",
    "                if(pred_image_class == true_image_class):\n",
    "                    correct_images_perclass = correct_images_perclass + 1\n",
    "                    total_class_votes = total_class_votes + votes\n",
    "                    total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                    \n",
    "                    total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                    z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                    for i in range(0,len(arr_pred_patches)):\n",
    "                        if(z[i]==1):\n",
    "                            s= s+1\n",
    "                            prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "            \n",
    "                arr_pred_patches = []\n",
    "                arr_pred_patches_prob = []\n",
    "                a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "            if(a==initial_filename):\n",
    "                total_patches_perclass = total_patches_perclass + 1\n",
    "                arr_pred_patches.append(pred_patch_class)\n",
    "                arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "    total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "    counts = np.bincount(arr_pred_patches)\n",
    "    pred_image_class = np.argmax(counts)\n",
    "    votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "    if(pred_image_class == true_image_class):\n",
    "        correct_images_perclass = correct_images_perclass + 1\n",
    "        total_class_votes = total_class_votes + votes\n",
    "        total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "        \n",
    "        total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "        z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "        for i in range(0,len(arr_pred_patches)):\n",
    "            if(z[i]==1):\n",
    "                prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "    arr_pred_patches = []\n",
    "    arr_pred_patches_prob = []\n",
    "    \n",
    "    if(correct_images_perclass!=0):\n",
    "        prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "        prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "        avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "        print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "    else:\n",
    "        with open(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'), 'a+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                             total_correc_img_patches,total_class_votes,\\\n",
    "                             avg_vote_perclass,\\\n",
    "                             prob_avg_correct_patch])\n",
    "            \n",
    "\n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results_Random.csv'))\n",
    "\n",
    "ILA_merged = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_merged}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ef36ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Level Accuracy (Trained on Merged Set, Tested on Random Set): 94.03846153846153%\n",
      "Image Level Accuracy (Trained on Merged Set, Tested on Similar Set): 95.72916666666667%\n",
      "Image Level Accuracy (Trained on Merged Set, Tested on Merged Set): 94.85%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image Level Accuracy (Trained on Merged Set, Tested on Random Set): {ILA_random}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Merged Set, Tested on Similar Set): {ILA_similar}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Merged Set, Tested on Merged Set): {ILA_merged}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece2b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
