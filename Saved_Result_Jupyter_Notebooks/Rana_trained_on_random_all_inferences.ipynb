{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b397aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.2+cu121\n",
      "Torchvision Version:  0.16.2+cu121\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torchvision import models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26666da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "   '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "   '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP','.mat',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "   return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "   classes = os.listdir(dir)\n",
    "   classes.sort()\n",
    "   class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "   return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "   images = []\n",
    "   for target in os.listdir(dir):\n",
    "       d = os.path.join(dir, target)\n",
    "       if not os.path.isdir(d):\n",
    "           continue\n",
    "\n",
    "       for filename in os.listdir(d):\n",
    "           if is_image_file(filename):\n",
    "               path = '{0}/{1}'.format(target, filename)\n",
    "               #print(path)\n",
    "               item = (path, class_to_idx[target])\n",
    "               images.append(item)\n",
    "\n",
    "   return images\n",
    "\n",
    "def default_loader(path):\n",
    "   return Image.open(path).convert('RGB')\n",
    "\n",
    "def mat_loader(path):\n",
    "   return scipy.io.loadmat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076a7f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 274176\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 245760\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 519936\n"
     ]
    }
   ],
   "source": [
    "classes1, class_to_idx1 = find_classes(\"/home/user1/icip/final_patch_ours_random/train/\")\n",
    "       \n",
    "imgs1 = make_dataset(\"/home/user1/icip/final_patch_ours_random/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/final_patch_ours_similar/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/final_patch_merged_ours/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecb935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderLoader(data.Dataset):\n",
    "   def __init__(self, root1,transform_1=None,\n",
    "                target_transform=None,\n",
    "                loader=default_loader):\n",
    "       classes1, class_to_idx1 = find_classes(root1)\n",
    "       \n",
    "       imgs1 = make_dataset(root1, class_to_idx1)\n",
    "      \n",
    "\n",
    "       self.root1 = root1\n",
    "       self.imgs1 = imgs1\n",
    "       self.classes1 = classes1\n",
    "       self.class_to_idx1 = class_to_idx1\n",
    "       self.transform_1 = transform_1\n",
    "       self.target_transform = target_transform\n",
    "       self.loader = loader\n",
    "        \n",
    "       self.img_noise = None\n",
    "       self.img_rgb = None\n",
    "    \n",
    "    \n",
    "   def SRM(self):\n",
    "    \n",
    "        imgs = self.img_rgb\n",
    "        \n",
    "        filter2 = [[0, 0, 0, 0, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 2, -4, 2, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 0, 0, 0, 0]]\n",
    "        # filter2ï¼šegde5*5\n",
    "        filter1 = [[-1, 2, -2, 2, -1],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-2, 8, -12, 8, -2],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-1, 2, -2, 2, -1]]\n",
    "        # filter3\n",
    "        filter3 = [[0, 0, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0,-2, 0, 0],\n",
    "                   [0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 0]]\n",
    "\n",
    "        filter1 = np.asarray(filter1, dtype=float) / 12\n",
    "        filter2 = np.asarray(filter2, dtype=float) / 4\n",
    "        filter3 = np.asarray(filter3, dtype=float) / 2\n",
    "\n",
    "        filters = [[filter1, filter1, filter1], [filter2, filter2, filter2], [filter3, filter3, filter3]]# (3,3,5,5)\n",
    "\n",
    "        filters = torch.FloatTensor(filters)  \n",
    "         # (3,3,5,5)\n",
    "        imgs = np.array(imgs, dtype=float)  # (375,500,3)\n",
    "        w,h,c = imgs.shape\n",
    "        imgs = imgs.reshape(1,w,h,c)\n",
    "        imgs = np.einsum('klij->kjli', imgs)\n",
    "\n",
    "        \n",
    "        input = torch.tensor(imgs, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        op1 = F.conv2d(input, filters, stride=1, padding=2)\n",
    "        #print('op1\\'s shape', op1.shape)\n",
    "\n",
    "        \n",
    "        op1= op1.reshape(c,w,h)\n",
    "    \n",
    "        self.img_noise = op1\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "       \n",
    "\n",
    "   def __getitem__(self, index):\n",
    "    \n",
    "\n",
    "       path1, target1 = self.imgs1[index]  \n",
    "       filename = Path(path1).stem \n",
    "       img1 = self.loader(os.path.join(self.root1, path1))  \n",
    "    \n",
    "       self.img_rgb = img1\n",
    "       \n",
    "       \n",
    "       self.SRM()\n",
    "    \n",
    "       #print(self.img_noise.shape)\n",
    "       \n",
    "    \n",
    "\n",
    "       if self.transform_1 is not None:\n",
    "           img1 = self.transform_1(self.img_rgb)\n",
    "        \n",
    "       if self.target_transform is not None:\n",
    "           target1 = self.target_transform(target)\n",
    "            \n",
    "       \n",
    "       \n",
    "       return img1,self.img_noise, target1,filename\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.imgs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ed7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batchsize=1\n",
    "\n",
    "val_dataset_random = ImageFolderLoader(\n",
    "        \"/home/user1/icip/final_patch_ours_random/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_random = torch.utils.data.DataLoader(\n",
    "        val_dataset_random, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_similar = ImageFolderLoader(\n",
    "        \"/home/user1/icip/final_patch_ours_similar/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_similar = torch.utils.data.DataLoader(\n",
    "        val_dataset_similar, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_merged = ImageFolderLoader(\n",
    "        \"/home/user1/icip/final_patch_merged_ours/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_merged = torch.utils.data.DataLoader(\n",
    "        val_dataset_merged, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48106372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274176, 245760, 519936)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset_random), len(val_dataset_similar), len(val_dataset_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a729f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        resnet1 = models.resnet50(pretrained=True)\n",
    "        modules1 = list(resnet1.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet1 = nn.Sequential(*modules1)\n",
    "\n",
    "        resnet2 = models.resnet50(pretrained=True)\n",
    "        modules2 = list(resnet2.children())[:-1] \n",
    "        self.resnet2 = nn.Sequential(*modules2)\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(2048*2, 2048)\n",
    "        self.fc2 = nn.Linear(2048,30)\n",
    "        \n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "        \n",
    "        x1 = self.resnet1(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "\n",
    "        x2 = self.resnet2(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "\n",
    "        x = torch.cat((x1,x2),dim=1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "       \n",
    "        out_fc = x\n",
    "        output = self.logsoftmax(x)\n",
    "        \n",
    "        return output, out_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bcd450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8cf10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing Results_Random folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_rnd_34_15',) 0 0 1.0\n",
      "1000 ('D01_rnd_63_230',) 0 0 1.0\n",
      "2000 ('D01_rnd_53_9',) 0 0 1.0\n",
      "3000 ('D01_rnd_34_253',) 0 0 0.99\n",
      "4000 ('D01_rnd_99_107',) 0 0 0.99\n",
      "5000 ('D01_rnd_150_210',) 0 0 1.0\n",
      "6000 ('D01_rnd_99_80',) 0 0 1.0\n",
      "7000 ('D01_rnd_150_69',) 0 0 1.0\n",
      "8000 ('D01_rnd_153_244',) 0 0 1.0\n",
      "Yes_Class 28\n",
      "9000 ('D29_rnd_146_246',) 28 28 1.0\n",
      "10000 ('D29_rnd_98_231',) 28 14 0.51\n",
      "11000 ('D29_rnd_2_42',) 28 28 1.0\n",
      "12000 ('D29_rnd_39_122',) 28 28 0.52\n",
      "13000 ('D29_rnd_113_229',) 28 28 1.0\n",
      "14000 ('D29_rnd_150_12',) 28 28 1.0\n",
      "15000 ('D29_rnd_141_4',) 28 28 1.0\n",
      "16000 ('D29_rnd_119_252',) 28 28 1.0\n",
      "17000 ('D29_rnd_119_231',) 28 28 1.0\n",
      "Yes_Class 22\n",
      "18000 ('D23_rnd_87_69',) 22 22 1.0\n",
      "19000 ('D23_rnd_135_79',) 22 22 1.0\n",
      "20000 ('D23_rnd_124_54',) 22 22 0.98\n",
      "21000 ('D23_rnd_70_54',) 22 22 1.0\n",
      "22000 ('D23_rnd_61_55',) 22 22 1.0\n",
      "23000 ('D23_rnd_135_55',) 22 22 1.0\n",
      "24000 ('D23_rnd_99_184',) 22 22 1.0\n",
      "25000 ('D23_rnd_113_241',) 22 22 1.0\n",
      "Yes_Class 27\n",
      "26000 ('D28_rnd_60_49',) 27 28 1.0\n",
      "27000 ('D28_rnd_98_41',) 27 27 1.0\n",
      "28000 ('D28_rnd_49_229',) 27 27 0.68\n",
      "29000 ('D28_rnd_7_42',) 27 5 1.0\n",
      "30000 ('D28_rnd_69_23',) 27 27 1.0\n",
      "31000 ('D28_rnd_60_61',) 27 27 0.99\n",
      "32000 ('D28_rnd_65_37',) 27 27 1.0\n",
      "33000 ('D28_rnd_113_71',) 27 28 1.0\n",
      "Yes_Class 3\n",
      "34000 ('D04_rnd_214_217',) 3 3 1.0\n",
      "35000 ('D04_rnd_99_238',) 3 3 1.0\n",
      "36000 ('D04_rnd_141_207',) 3 3 1.0\n",
      "37000 ('D04_rnd_79_164',) 3 3 1.0\n",
      "38000 ('D04_rnd_130_28',) 3 3 1.0\n",
      "39000 ('D04_rnd_55_32',) 3 3 1.0\n",
      "40000 ('D04_rnd_124_179',) 3 3 1.0\n",
      "41000 ('D04_rnd_29_241',) 3 3 1.0\n",
      "42000 ('D04_rnd_146_220',) 3 3 1.0\n",
      "43000 ('D04_rnd_55_69',) 3 3 1.0\n",
      "44000 ('D04_rnd_153_36',) 3 3 1.0\n",
      "45000 ('D04_rnd_235_12',) 3 3 1.0\n",
      "46000 ('D04_rnd_185_144',) 3 3 1.0\n",
      "Yes_Class 14\n",
      "47000 ('D15_rnd_41_144',) 14 14 1.0\n",
      "48000 ('D15_rnd_99_138',) 14 14 1.0\n",
      "49000 ('D15_rnd_277_235',) 14 14 1.0\n",
      "50000 ('D15_rnd_109_230',) 14 14 1.0\n",
      "51000 ('D15_rnd_101_158',) 14 14 1.0\n",
      "52000 ('D15_rnd_117_25',) 14 14 1.0\n",
      "53000 ('D15_rnd_162_157',) 14 14 1.0\n",
      "54000 ('D15_rnd_59_76',) 14 14 1.0\n",
      "55000 ('D15_rnd_171_121',) 14 14 1.0\n",
      "56000 ('D15_rnd_221_27',) 14 14 1.0\n",
      "57000 ('D15_rnd_194_213',) 14 14 1.0\n",
      "58000 ('D15_rnd_130_165',) 14 14 1.0\n",
      "59000 ('D15_rnd_136_253',) 14 14 1.0\n",
      "60000 ('D15_rnd_221_242',) 14 14 1.0\n",
      "Yes_Class 20\n",
      "61000 ('D21_rnd_130_66',) 20 1 0.71\n",
      "62000 ('D21_rnd_65_213',) 20 20 1.0\n",
      "63000 ('D21_rnd_67_30',) 20 20 1.0\n",
      "64000 ('D21_rnd_49_83',) 20 20 0.76\n",
      "65000 ('D21_rnd_49_158',) 20 19 0.49\n",
      "66000 ('D21_rnd_132_92',) 20 20 0.94\n",
      "67000 ('D21_rnd_132_212',) 20 20 1.0\n",
      "68000 ('D21_rnd_79_21',) 20 20 1.0\n",
      "Yes_Class 4\n",
      "69000 ('D05_rnd_89_167',) 4 4 1.0\n",
      "70000 ('D05_rnd_117_211',) 4 4 1.0\n",
      "71000 ('D05_rnd_44_25',) 4 4 1.0\n",
      "72000 ('D05_rnd_10_62',) 4 4 1.0\n",
      "73000 ('D05_rnd_113_4',) 4 4 1.0\n",
      "74000 ('D05_rnd_63_206',) 4 4 1.0\n",
      "75000 ('D05_rnd_87_163',) 4 4 1.0\n",
      "76000 ('D05_rnd_89_85',) 4 4 1.0\n",
      "77000 ('D05_rnd_44_231',) 4 4 1.0\n",
      "Yes_Class 18\n",
      "78000 ('D19_rnd_138_28',) 18 18 1.0\n",
      "79000 ('D19_rnd_48_17',) 18 18 1.0\n",
      "80000 ('D19_rnd_113_167',) 18 18 1.0\n",
      "81000 ('D19_rnd_153_124',) 18 18 1.0\n",
      "82000 ('D19_rnd_45_201',) 18 18 1.0\n",
      "83000 ('D19_rnd_122_67',) 18 18 1.0\n",
      "84000 ('D19_rnd_108_125',) 18 18 1.0\n",
      "85000 ('D19_rnd_36_66',) 18 18 1.0\n",
      "86000 ('D19_rnd_117_102',) 18 18 1.0\n",
      "87000 ('D19_rnd_185_57',) 18 18 1.0\n",
      "88000 ('D19_rnd_50_39',) 18 18 1.0\n",
      "Yes_Class 17\n",
      "89000 ('D18_rnd_57_135',) 17 17 1.0\n",
      "90000 ('D18_rnd_135_8',) 17 17 1.0\n",
      "91000 ('D18_rnd_130_190',) 17 17 1.0\n",
      "92000 ('D18_rnd_94_72',) 17 17 1.0\n",
      "93000 ('D18_rnd_40_10',) 17 17 1.0\n",
      "94000 ('D18_rnd_85_131',) 17 4 0.76\n",
      "95000 ('D18_rnd_57_60',) 17 17 1.0\n",
      "96000 ('D18_rnd_10_21',) 17 24 0.95\n",
      "Yes_Class 10\n",
      "97000 ('D11_rnd_9_213',) 10 10 1.0\n",
      "98000 ('D11_rnd_166_185',) 10 10 1.0\n",
      "99000 ('D11_rnd_84_50',) 10 10 1.0\n",
      "100000 ('D11_rnd_135_60',) 10 17 1.0\n",
      "101000 ('D11_rnd_86_176',) 10 10 1.0\n",
      "102000 ('D11_rnd_117_107',) 10 10 1.0\n",
      "103000 ('D11_rnd_10_211',) 10 10 1.0\n",
      "104000 ('D11_rnd_146_188',) 10 10 1.0\n",
      "105000 ('D11_rnd_21_198',) 10 10 1.0\n",
      "Yes_Class 12\n",
      "106000 ('D13_rnd_35_128',) 12 12 1.0\n",
      "107000 ('D13_rnd_63_99',) 12 12 0.98\n",
      "108000 ('D13_rnd_124_73',) 12 12 1.0\n",
      "109000 ('D13_rnd_10_115',) 12 12 1.0\n",
      "110000 ('D13_rnd_35_20',) 12 12 1.0\n",
      "111000 ('D13_rnd_86_248',) 12 12 1.0\n",
      "112000 ('D13_rnd_153_162',) 12 12 1.0\n",
      "113000 ('D13_rnd_10_40',) 12 12 1.0\n",
      "114000 ('D13_rnd_122_113',) 12 12 1.0\n",
      "Yes_Class 19\n",
      "115000 ('D20_rnd_43_255',) 19 19 1.0\n",
      "116000 ('D20_rnd_61_216',) 19 19 1.0\n",
      "117000 ('D20_rnd_129_203',) 19 19 1.0\n",
      "118000 ('D20_rnd_138_252',) 19 19 1.0\n",
      "119000 ('D20_rnd_96_189',) 19 19 0.98\n",
      "120000 ('D20_rnd_96_138',) 19 20 1.0\n",
      "121000 ('D20_rnd_21_50',) 19 19 1.0\n",
      "122000 ('D20_rnd_85_32',) 19 19 1.0\n",
      "Yes_Class 5\n",
      "123000 ('D06_rnd_48_141',) 5 5 0.96\n",
      "124000 ('D06_rnd_81_67',) 5 5 1.0\n",
      "125000 ('D06_rnd_10_213',) 5 5 1.0\n",
      "126000 ('D06_rnd_98_133',) 5 5 1.0\n",
      "127000 ('D06_rnd_16_126',) 5 5 1.0\n",
      "128000 ('D06_rnd_64_21',) 5 5 1.0\n",
      "129000 ('D06_rnd_69_76',) 5 5 1.0\n",
      "130000 ('D06_rnd_135_127',) 5 27 0.95\n",
      "Yes_Class 1\n",
      "131000 ('D02_rnd_99_179',) 1 1 1.0\n",
      "132000 ('D02_rnd_39_46',) 1 1 1.0\n",
      "133000 ('D02_rnd_141_77',) 1 1 1.0\n",
      "134000 ('D02_rnd_188_169',) 1 1 1.0\n",
      "135000 ('D02_rnd_39_202',) 1 10 0.99\n",
      "136000 ('D02_rnd_65_198',) 1 1 1.0\n",
      "137000 ('D02_rnd_113_214',) 1 1 1.0\n",
      "138000 ('D02_rnd_177_76',) 1 1 1.0\n",
      "139000 ('D02_rnd_129_220',) 1 1 1.0\n",
      "140000 ('D02_rnd_44_135',) 1 1 1.0\n",
      "141000 ('D02_rnd_187_255',) 1 1 1.0\n",
      "Yes_Class 2\n",
      "142000 ('D03_rnd_197_3',) 2 2 1.0\n",
      "143000 ('D03_rnd_108_135',) 2 2 1.0\n",
      "144000 ('D03_rnd_63_207',) 2 2 1.0\n",
      "145000 ('D03_rnd_226_164',) 2 2 1.0\n",
      "146000 ('D03_rnd_150_247',) 2 2 1.0\n",
      "147000 ('D03_rnd_59_53',) 2 2 1.0\n",
      "148000 ('D03_rnd_205_131',) 2 2 1.0\n",
      "149000 ('D03_rnd_215_15',) 2 2 1.0\n",
      "150000 ('D03_rnd_122_200',) 2 2 1.0\n",
      "151000 ('D03_rnd_27_124',) 2 2 1.0\n",
      "152000 ('D03_rnd_58_29',) 2 2 1.0\n",
      "153000 ('D03_rnd_188_96',) 2 2 1.0\n",
      "Yes_Class 21\n",
      "154000 ('D22_rnd_53_1',) 21 21 1.0\n",
      "155000 ('D22_rnd_6_57',) 21 21 1.0\n",
      "156000 ('D22_rnd_110_81',) 21 21 1.0\n",
      "157000 ('D22_rnd_4_222',) 21 21 1.0\n",
      "158000 ('D22_rnd_98_211',) 21 21 1.0\n",
      "159000 ('D22_rnd_108_1',) 21 21 1.0\n",
      "160000 ('D22_rnd_6_74',) 21 21 1.0\n",
      "161000 ('D22_rnd_110_172',) 21 21 1.0\n",
      "Yes_Class 23\n",
      "162000 ('D24_rnd_122_8',) 23 23 1.0\n",
      "163000 ('D24_rnd_129_70',) 23 23 1.0\n",
      "164000 ('D24_rnd_153_73',) 23 23 1.0\n",
      "165000 ('D24_rnd_18_204',) 23 23 1.0\n",
      "166000 ('D24_rnd_60_21',) 23 23 1.0\n",
      "167000 ('D24_rnd_10_41',) 23 23 1.0\n",
      "168000 ('D24_rnd_138_217',) 23 23 1.0\n",
      "169000 ('D24_rnd_129_89',) 23 23 1.0\n",
      "Yes_Class 25\n",
      "170000 ('D26_rnd_85_181',) 25 25 1.0\n",
      "171000 ('D26_rnd_141_68',) 25 25 1.0\n",
      "172000 ('D26_rnd_124_38',) 25 25 1.0\n",
      "173000 ('D26_rnd_10_192',) 25 25 1.0\n",
      "174000 ('D26_rnd_40_38',) 25 25 1.0\n",
      "175000 ('D26_rnd_122_162',) 25 25 1.0\n",
      "176000 ('D26_rnd_59_244',) 25 25 1.0\n",
      "177000 ('D26_rnd_141_121',) 25 25 1.0\n",
      "178000 ('D26_rnd_160_100',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "179000 ('D10_rnd_129_138',) 9 9 1.0\n",
      "180000 ('D10_rnd_99_63',) 9 9 1.0\n",
      "181000 ('D10_rnd_141_189',) 9 9 1.0\n",
      "182000 ('D10_rnd_150_29',) 9 9 1.0\n",
      "183000 ('D10_rnd_6_216',) 9 15 1.0\n",
      "184000 ('D10_rnd_17_102',) 9 9 1.0\n",
      "185000 ('D10_rnd_153_201',) 9 9 1.0\n",
      "186000 ('D10_rnd_97_62',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "187000 ('D30_rnd_99_153',) 29 29 1.0\n",
      "188000 ('D30_rnd_10_17',) 29 29 0.99\n",
      "189000 ('D30_rnd_39_111',) 29 19 0.88\n",
      "190000 ('D30_rnd_37_32',) 29 29 1.0\n",
      "191000 ('D30_rnd_99_50',) 29 29 1.0\n",
      "192000 ('D30_rnd_37_253',) 29 29 1.0\n",
      "193000 ('D30_rnd_129_44',) 29 29 1.0\n",
      "194000 ('D30_rnd_92_217',) 29 29 1.0\n",
      "195000 ('D30_rnd_146_145',) 29 29 1.0\n",
      "Yes_Class 7\n",
      "196000 ('D08_rnd_130_67',) 7 7 1.0\n",
      "197000 ('D08_rnd_117_91',) 7 7 1.0\n",
      "198000 ('D08_rnd_196_192',) 7 7 1.0\n",
      "199000 ('D08_rnd_166_75',) 7 7 1.0\n",
      "200000 ('D08_rnd_135_183',) 7 7 1.0\n",
      "201000 ('D08_rnd_150_102',) 7 7 0.95\n",
      "202000 ('D08_rnd_39_255',) 7 7 1.0\n",
      "203000 ('D08_rnd_113_212',) 7 7 0.99\n",
      "204000 ('D08_rnd_153_116',) 7 7 1.0\n",
      "205000 ('D08_rnd_176_66',) 7 7 1.0\n",
      "Yes_Class 13\n",
      "206000 ('D14_rnd_39_70',) 13 13 1.0\n",
      "207000 ('D14_rnd_146_115',) 13 13 1.0\n",
      "208000 ('D14_rnd_39_236',) 13 13 1.0\n",
      "209000 ('D14_rnd_57_215',) 13 13 1.0\n",
      "210000 ('D14_rnd_150_211',) 13 13 1.0\n",
      "211000 ('D14_rnd_45_164',) 13 13 1.0\n",
      "212000 ('D14_rnd_18_94',) 13 10 1.0\n",
      "213000 ('D14_rnd_108_165',) 13 13 1.0\n",
      "214000 ('D14_rnd_108_121',) 13 13 1.0\n",
      "Yes_Class 6\n",
      "215000 ('D07_rnd_108_198',) 6 6 1.0\n",
      "216000 ('D07_rnd_83_38',) 6 6 1.0\n",
      "217000 ('D07_rnd_160_107',) 6 6 1.0\n",
      "218000 ('D07_rnd_119_46',) 6 6 1.0\n",
      "219000 ('D07_rnd_83_16',) 6 6 1.0\n",
      "220000 ('D07_rnd_124_175',) 6 6 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221000 ('D07_rnd_176_216',) 6 6 1.0\n",
      "222000 ('D07_rnd_24_32',) 6 6 1.0\n",
      "223000 ('D07_rnd_108_207',) 6 6 1.0\n",
      "Yes_Class 16\n",
      "224000 ('D17_rnd_98_206',) 16 16 1.0\n",
      "225000 ('D17_rnd_64_131',) 16 16 1.0\n",
      "226000 ('D17_rnd_124_196',) 16 16 1.0\n",
      "227000 ('D17_rnd_110_173',) 16 16 1.0\n",
      "228000 ('D17_rnd_135_98',) 16 16 1.0\n",
      "229000 ('D17_rnd_113_100',) 16 16 1.0\n",
      "230000 ('D17_rnd_110_137',) 16 16 1.0\n",
      "231000 ('D17_rnd_10_121',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "232000 ('D27_rnd_47_94',) 26 26 1.0\n",
      "233000 ('D27_rnd_2_172',) 26 26 1.0\n",
      "234000 ('D27_rnd_99_205',) 26 26 1.0\n",
      "235000 ('D27_rnd_99_86',) 26 26 1.0\n",
      "236000 ('D27_rnd_75_173',) 26 26 1.0\n",
      "237000 ('D27_rnd_153_254',) 26 26 1.0\n",
      "238000 ('D27_rnd_99_201',) 26 26 1.0\n",
      "239000 ('D27_rnd_47_116',) 26 26 1.0\n",
      "Yes_Class 11\n",
      "240000 ('D12_rnd_117_180',) 11 11 1.0\n",
      "241000 ('D12_rnd_129_204',) 11 11 1.0\n",
      "242000 ('D12_rnd_71_121',) 11 11 1.0\n",
      "243000 ('D12_rnd_176_87',) 11 11 1.0\n",
      "244000 ('D12_rnd_150_124',) 11 11 1.0\n",
      "245000 ('D12_rnd_176_102',) 11 11 1.0\n",
      "246000 ('D12_rnd_70_195',) 11 11 1.0\n",
      "247000 ('D12_rnd_27_215',) 11 11 1.0\n",
      "248000 ('D12_rnd_71_234',) 11 11 1.0\n",
      "249000 ('D12_rnd_66_7',) 11 11 1.0\n",
      "Yes_Class 15\n",
      "250000 ('D16_rnd_122_87',) 15 15 1.0\n",
      "251000 ('D16_rnd_86_35',) 15 15 1.0\n",
      "252000 ('D16_rnd_130_79',) 15 15 1.0\n",
      "253000 ('D16_rnd_19_29',) 15 15 1.0\n",
      "254000 ('D16_rnd_110_156',) 15 15 1.0\n",
      "255000 ('D16_rnd_34_149',) 15 15 1.0\n",
      "256000 ('D16_rnd_46_250',) 15 15 1.0\n",
      "257000 ('D16_rnd_146_89',) 15 15 1.0\n",
      "Yes_Class 24\n",
      "258000 ('D25_rnd_65_26',) 24 24 1.0\n",
      "259000 ('D25_rnd_86_133',) 24 24 1.0\n",
      "260000 ('D25_rnd_89_83',) 24 24 1.0\n",
      "261000 ('D25_rnd_89_15',) 24 24 1.0\n",
      "262000 ('D25_rnd_38_111',) 24 24 1.0\n",
      "263000 ('D25_rnd_117_200',) 24 17 1.0\n",
      "264000 ('D25_rnd_5_213',) 24 24 1.0\n",
      "265000 ('D25_rnd_86_27',) 24 1 0.79\n",
      "Yes_Class 8\n",
      "266000 ('D09_rnd_25_39',) 8 8 1.0\n",
      "267000 ('D09_rnd_150_117',) 8 8 1.0\n",
      "268000 ('D09_rnd_24_155',) 8 8 1.0\n",
      "269000 ('D09_rnd_166_78',) 8 8 1.0\n",
      "270000 ('D09_rnd_129_251',) 8 8 1.0\n",
      "271000 ('D09_rnd_166_51',) 8 8 1.0\n",
      "272000 ('D09_rnd_122_231',) 8 8 1.0\n",
      "273000 ('D09_rnd_44_35',) 8 8 0.99\n",
      "274000 ('D09_rnd_63_42',) 8 8 1.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"Rana_trained_model_random_final\")\n",
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Random\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Random folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,imgs2,labels1,patch_filename) in enumerate(test_loader_random):\n",
    "        c = labels1\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org, mat_img, target = imgs1.to(device),imgs2.to(device), labels1.to(device)\n",
    "        output, fc_feature = model(img_org,mat_img)\n",
    "        \n",
    "        output = F.softmax(fc_feature,dim=1)\n",
    "        \n",
    "\n",
    "        actual = target\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d9ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Random/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18c0fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 9216 36 8543 92.7 0.99\n",
      "32 8192 32 8005 97.72 1.0\n",
      "43 11008 43 10934 99.33 1.0\n",
      "38 9728 37 9266 97.83 0.99\n",
      "32 8192 32 7813 95.37 0.99\n",
      "34 8704 34 8523 97.92 1.0\n",
      "32 8192 32 7957 97.13 1.0\n",
      "40 10240 39 9817 98.33 1.0\n",
      "44 11264 44 10742 95.37 0.99\n",
      "47 12032 47 11791 98.0 1.0\n",
      "35 8960 33 8143 96.39 0.99\n",
      "31 7936 31 7685 96.84 1.0\n",
      "34 8704 34 8234 94.6 0.99\n",
      "34 8704 34 8464 97.24 0.99\n",
      "33 8448 33 8388 99.29 1.0\n",
      "31 7936 31 7329 92.35 0.99\n",
      "34 8704 34 8463 97.23 0.99\n",
      "31 7936 31 7155 90.16 0.99\n",
      "31 7936 31 7831 98.68 1.0\n",
      "56 14336 56 13813 96.35 0.99\n",
      "51 13056 51 13030 99.8 1.0\n",
      "33 8448 33 7991 94.59 0.99\n",
      "31 7936 30 7038 91.64 0.99\n",
      "33 8448 33 8073 95.56 0.99\n",
      "31 7936 27 5918 85.62 0.98\n",
      "30 7680 30 6989 91.0 0.98\n",
      "32 8192 31 7616 95.97 0.99\n",
      "32 8192 32 7943 96.96 0.99\n",
      "35 8960 34 8304 95.4 0.99\n",
      "35 8960 34 8425 96.79 0.99\n",
      "Image Level Accuracy: 98.8795518207283%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "      df = pd.read_csv(f)\n",
    "      data = df.sort_values(by=['Patch_Filename'])\n",
    "      classname = Path(f).stem\n",
    "      classname = int(classname.split(\"_\")[2])\n",
    "      a=\"a\"\n",
    "\n",
    "      true_image_class = classname\n",
    "      total_images_perclass = 0\n",
    "      correct_images_perclass = 0\n",
    "      total_patches_perclass = 0\n",
    "\n",
    "      total_class_votes =0\n",
    "      total_class_patches =0\n",
    "      prob_avg_correct_patch = 0.0\n",
    "      votes = 0\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "      for ind in data.index:\n",
    "              filename = df['Patch_Filename'][ind]\n",
    "              pred_patch_class = df['Predicted Class'][ind]\n",
    "              pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "              \n",
    "              filename = filename.split(\"_\")\n",
    "              file_length = len(filename)\n",
    "              initial_filename = filename[:-1]\n",
    "              patch_name = filename[file_length-1]\n",
    "              #print(initial_filename)\n",
    "              #print(patch_name)\n",
    "\n",
    "              if(a!= initial_filename and a==\"a\"):\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a!=initial_filename and a!=\"a\"):\n",
    "                  total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                  counts = np.bincount(arr_pred_patches)\n",
    "                  pred_image_class = np.argmax(counts)\n",
    "                  votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                  s=0\n",
    "                  \n",
    "                  if(pred_image_class == true_image_class):\n",
    "                      correct_images_perclass = correct_images_perclass + 1\n",
    "                      total_class_votes = total_class_votes + votes\n",
    "                      total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                      \n",
    "                      total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                      z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                      for i in range(0,len(arr_pred_patches)):\n",
    "                          if(z[i]==1):\n",
    "                              s= s+1\n",
    "                              prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "              \n",
    "                  arr_pred_patches = []\n",
    "                  arr_pred_patches_prob = []\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a==initial_filename):\n",
    "                  total_patches_perclass = total_patches_perclass + 1\n",
    "                  arr_pred_patches.append(pred_patch_class)\n",
    "                  arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "      total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "      counts = np.bincount(arr_pred_patches)\n",
    "      pred_image_class = np.argmax(counts)\n",
    "      votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "      if(pred_image_class == true_image_class):\n",
    "          correct_images_perclass = correct_images_perclass + 1\n",
    "          total_class_votes = total_class_votes + votes\n",
    "          total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "          \n",
    "          total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "          z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "          for i in range(0,len(arr_pred_patches)):\n",
    "              if(z[i]==1):\n",
    "                  prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      \n",
    "      if(correct_images_perclass!=0):\n",
    "          prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "          prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "          avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "          print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "      else:\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "              \n",
    "              \n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results.csv'))\n",
    "\n",
    "ILA_random = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_random}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad359b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing Results_Similar folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_color_2_167',) 0 0 1.0\n",
      "1000 ('D01_obj_29_58',) 0 3 0.94\n",
      "2000 ('D01_tex_25_97',) 0 15 1.0\n",
      "3000 ('D01_nat_2_131',) 0 0 1.0\n",
      "4000 ('D01_nat_39_61',) 0 0 1.0\n",
      "5000 ('D01_nat_1_8',) 0 0 1.0\n",
      "6000 ('D01_tex_36_229',) 0 26 0.99\n",
      "7000 ('D01_obj_36_26',) 0 28 1.0\n",
      "8000 ('D01_tex_25_239',) 0 15 1.0\n",
      "Yes_Class 28\n",
      "9000 ('D29_obj_17_207',) 28 6 0.72\n",
      "10000 ('D29_nat_9_82',) 28 10 0.84\n",
      "11000 ('D29_tex_39_254',) 28 15 0.81\n",
      "12000 ('D29_nat_9_109',) 28 28 0.98\n",
      "13000 ('D29_nat_49_206',) 28 10 1.0\n",
      "14000 ('D29_obj_17_86',) 28 5 0.99\n",
      "15000 ('D29_obj_36_109',) 28 28 1.0\n",
      "16000 ('D29_obj_30_181',) 28 27 0.96\n",
      "Yes_Class 22\n",
      "17000 ('D23_color_14_164',) 22 22 1.0\n",
      "18000 ('D23_tex_25_176',) 22 22 1.0\n",
      "19000 ('D23_obj_36_156',) 22 22 0.7\n",
      "20000 ('D23_nat_17_19',) 22 22 1.0\n",
      "21000 ('D23_obj_36_203',) 22 22 1.0\n",
      "22000 ('D23_obj_2_17',) 22 22 1.0\n",
      "23000 ('D23_color_14_94',) 22 26 0.68\n",
      "24000 ('D23_nat_30_231',) 22 22 0.98\n",
      "Yes_Class 27\n",
      "25000 ('D28_nat_9_129',) 27 5 0.5\n",
      "26000 ('D28_nat_29_191',) 27 27 0.98\n",
      "27000 ('D28_nat_30_9',) 27 28 0.97\n",
      "28000 ('D28_nat_30_202',) 27 27 1.0\n",
      "29000 ('D28_obj_39_147',) 27 27 1.0\n",
      "30000 ('D28_nat_1_236',) 27 5 1.0\n",
      "31000 ('D28_color_4_210',) 27 27 1.0\n",
      "32000 ('D28_nat_49_242',) 27 27 1.0\n",
      "Yes_Class 3\n",
      "33000 ('D04_obj_25_232',) 3 3 1.0\n",
      "34000 ('D04_nat_1_87',) 3 3 1.0\n",
      "35000 ('D04_obj_2_211',) 3 28 0.63\n",
      "36000 ('D04_nat_39_37',) 3 3 1.0\n",
      "37000 ('D04_obj_30_151',) 3 3 0.73\n",
      "38000 ('D04_tex_36_65',) 3 3 1.0\n",
      "39000 ('D04_obj_30_249',) 3 28 1.0\n",
      "40000 ('D04_tex_2_158',) 3 27 0.82\n",
      "Yes_Class 14\n",
      "41000 ('D15_obj_25_1',) 14 14 1.0\n",
      "42000 ('D15_nat_49_204',) 14 14 1.0\n",
      "43000 ('D15_nat_49_243',) 14 14 1.0\n",
      "44000 ('D15_obj_25_174',) 14 14 1.0\n",
      "45000 ('D15_obj_2_249',) 14 14 0.87\n",
      "46000 ('D15_nat_17_232',) 14 14 1.0\n",
      "47000 ('D15_color_4_110',) 14 14 1.0\n",
      "48000 ('D15_nat_29_233',) 14 14 1.0\n",
      "49000 ('D15_color_4_229',) 14 14 1.0\n",
      "Yes_Class 20\n",
      "50000 ('D21_obj_2_53',) 20 28 1.0\n",
      "51000 ('D21_nat_45_144',) 20 20 1.0\n",
      "52000 ('D21_tex_7_58',) 20 20 1.0\n",
      "53000 ('D21_tex_30_10',) 20 20 0.99\n",
      "54000 ('D21_tex_2_253',) 20 20 1.0\n",
      "55000 ('D21_nat_49_31',) 20 20 1.0\n",
      "56000 ('D21_obj_5_9',) 20 15 0.83\n",
      "57000 ('D21_nat_29_203',) 20 20 1.0\n",
      "Yes_Class 4\n",
      "58000 ('D05_tex_30_76',) 4 20 0.79\n",
      "59000 ('D05_nat_25_242',) 4 4 1.0\n",
      "60000 ('D05_obj_5_203',) 4 4 1.0\n",
      "61000 ('D05_nat_45_120',) 4 4 1.0\n",
      "62000 ('D05_color_2_104',) 4 1 0.86\n",
      "63000 ('D05_nat_49_248',) 4 4 1.0\n",
      "64000 ('D05_nat_25_163',) 4 4 1.0\n",
      "65000 ('D05_tex_2_54',) 4 10 0.75\n",
      "Yes_Class 18\n",
      "66000 ('D19_tex_29_81',) 18 18 1.0\n",
      "67000 ('D19_nat_30_24',) 18 26 0.83\n",
      "68000 ('D19_obj_36_195',) 18 17 1.0\n",
      "69000 ('D19_tex_30_15',) 18 18 1.0\n",
      "70000 ('D19_nat_1_97',) 18 18 1.0\n",
      "71000 ('D19_color_4_210',) 18 18 1.0\n",
      "72000 ('D19_nat_9_214',) 18 18 1.0\n",
      "73000 ('D19_obj_5_89',) 18 18 0.87\n",
      "Yes_Class 17\n",
      "74000 ('D18_nat_45_255',) 17 24 0.85\n",
      "75000 ('D18_nat_45_101',) 17 24 1.0\n",
      "76000 ('D18_tex_36_208',) 17 17 1.0\n",
      "77000 ('D18_nat_29_3',) 17 17 1.0\n",
      "78000 ('D18_nat_36_175',) 17 17 1.0\n",
      "79000 ('D18_tex_1_69',) 17 24 1.0\n",
      "80000 ('D18_obj_30_188',) 17 17 1.0\n",
      "81000 ('D18_nat_2_87',) 17 17 0.39\n",
      "Yes_Class 10\n",
      "82000 ('D11_nat_36_82',) 10 10 1.0\n",
      "83000 ('D11_tex_7_25',) 10 10 1.0\n",
      "84000 ('D11_nat_36_235',) 10 10 1.0\n",
      "85000 ('D11_nat_29_193',) 10 10 1.0\n",
      "86000 ('D11_tex_17_141',) 10 10 1.0\n",
      "87000 ('D11_tex_36_135',) 10 10 1.0\n",
      "88000 ('D11_nat_30_166',) 10 10 0.8\n",
      "89000 ('D11_obj_29_18',) 10 10 1.0\n",
      "90000 ('D11_obj_2_171',) 10 10 1.0\n",
      "Yes_Class 12\n",
      "91000 ('D13_obj_39_87',) 12 12 1.0\n",
      "92000 ('D13_nat_39_84',) 12 12 1.0\n",
      "93000 ('D13_nat_29_114',) 12 12 1.0\n",
      "94000 ('D13_tex_30_165',) 12 26 0.99\n",
      "95000 ('D13_obj_29_178',) 12 12 1.0\n",
      "96000 ('D13_nat_29_239',) 12 12 1.0\n",
      "97000 ('D13_nat_30_117',) 12 19 0.99\n",
      "98000 ('D13_nat_36_49',) 12 12 0.49\n",
      "Yes_Class 19\n",
      "99000 ('D20_nat_2_84',) 19 19 0.99\n",
      "100000 ('D20_obj_36_140',) 19 19 1.0\n",
      "101000 ('D20_obj_17_82',) 19 19 1.0\n",
      "102000 ('D20_nat_29_143',) 19 19 1.0\n",
      "103000 ('D20_obj_2_12',) 19 19 1.0\n",
      "104000 ('D20_color_4_188',) 19 19 1.0\n",
      "105000 ('D20_nat_9_83',) 19 1 0.52\n",
      "106000 ('D20_tex_1_31',) 19 19 1.0\n",
      "Yes_Class 5\n",
      "107000 ('D06_obj_29_115',) 5 5 0.88\n",
      "108000 ('D06_obj_25_235',) 5 5 1.0\n",
      "109000 ('D06_tex_25_203',) 5 27 1.0\n",
      "110000 ('D06_obj_17_113',) 5 27 0.66\n",
      "111000 ('D06_tex_25_59',) 5 27 1.0\n",
      "112000 ('D06_nat_25_15',) 5 5 1.0\n",
      "113000 ('D06_nat_49_202',) 5 5 1.0\n",
      "114000 ('D06_tex_30_88',) 5 5 0.99\n",
      "Yes_Class 1\n",
      "115000 ('D02_tex_25_241',) 1 1 1.0\n",
      "116000 ('D02_color_2_52',) 1 3 1.0\n",
      "117000 ('D02_obj_5_214',) 1 1 1.0\n",
      "118000 ('D02_nat_39_246',) 1 1 0.59\n",
      "119000 ('D02_tex_17_129',) 1 26 0.87\n",
      "120000 ('D02_nat_9_87',) 1 1 1.0\n",
      "121000 ('D02_nat_9_102',) 1 1 0.97\n",
      "122000 ('D02_obj_5_249',) 1 1 1.0\n",
      "Yes_Class 2\n",
      "123000 ('D03_nat_38_149',) 2 2 1.0\n",
      "124000 ('D03_tex_25_228',) 2 2 1.0\n",
      "125000 ('D03_color_2_21',) 2 2 1.0\n",
      "126000 ('D03_color_2_157',) 2 2 1.0\n",
      "127000 ('D03_nat_45_154',) 2 2 0.98\n",
      "128000 ('D03_tex_36_132',) 2 2 0.98\n",
      "129000 ('D03_nat_45_232',) 2 2 1.0\n",
      "130000 ('D03_tex_39_13',) 2 10 0.98\n",
      "131000 ('D03_nat_38_191',) 2 2 1.0\n",
      "Yes_Class 21\n",
      "132000 ('D22_nat_9_154',) 21 21 1.0\n",
      "133000 ('D22_obj_36_154',) 21 21 1.0\n",
      "134000 ('D22_obj_2_180',) 21 21 1.0\n",
      "135000 ('D22_obj_39_137',) 21 21 1.0\n",
      "136000 ('D22_nat_9_221',) 21 21 1.0\n",
      "137000 ('D22_color_2_226',) 21 21 0.98\n",
      "138000 ('D22_tex_1_138',) 21 21 1.0\n",
      "139000 ('D22_obj_2_159',) 21 12 1.0\n",
      "Yes_Class 23\n",
      "140000 ('D24_tex_7_0',) 23 15 0.43\n",
      "141000 ('D24_color_2_183',) 23 7 0.57\n",
      "142000 ('D24_tex_7_12',) 23 29 1.0\n",
      "143000 ('D24_tex_36_50',) 23 23 1.0\n",
      "144000 ('D24_color_4_192',) 23 23 0.78\n",
      "145000 ('D24_nat_1_176',) 23 29 0.98\n",
      "146000 ('D24_nat_49_212',) 23 19 0.41\n",
      "147000 ('D24_tex_7_244',) 23 26 1.0\n",
      "Yes_Class 25\n",
      "148000 ('D26_nat_2_180',) 25 25 1.0\n",
      "149000 ('D26_obj_36_173',) 25 7 0.55\n",
      "150000 ('D26_nat_25_30',) 25 25 1.0\n",
      "151000 ('D26_tex_2_240',) 25 25 1.0\n",
      "152000 ('D26_obj_29_131',) 25 25 0.91\n",
      "153000 ('D26_obj_30_58',) 25 25 0.95\n",
      "154000 ('D26_tex_39_36',) 25 25 1.0\n",
      "155000 ('D26_obj_5_240',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "156000 ('D10_obj_36_199',) 9 9 1.0\n",
      "157000 ('D10_nat_36_34',) 9 9 1.0\n",
      "158000 ('D10_color_14_246',) 9 9 1.0\n",
      "159000 ('D10_nat_36_41',) 9 9 1.0\n",
      "160000 ('D10_obj_5_151',) 9 9 1.0\n",
      "161000 ('D10_tex_36_202',) 9 9 1.0\n",
      "162000 ('D10_nat_30_250',) 9 9 1.0\n",
      "163000 ('D10_tex_30_252',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "164000 ('D30_obj_5_105',) 29 29 1.0\n",
      "165000 ('D30_nat_25_37',) 29 29 1.0\n",
      "166000 ('D30_obj_1_14',) 29 29 1.0\n",
      "167000 ('D30_nat_25_25',) 29 29 0.82\n",
      "168000 ('D30_color_4_125',) 29 10 0.98\n",
      "169000 ('D30_tex_29_160',) 29 29 1.0\n",
      "170000 ('D30_obj_17_219',) 29 29 1.0\n",
      "171000 ('D30_obj_2_9',) 29 28 0.95\n",
      "172000 ('D30_color_2_10',) 29 10 0.61\n",
      "Yes_Class 7\n",
      "173000 ('D08_obj_2_25',) 7 7 0.99\n",
      "174000 ('D08_color_14_27',) 7 7 1.0\n",
      "175000 ('D08_obj_25_176',) 7 7 1.0\n",
      "176000 ('D08_tex_1_141',) 7 7 1.0\n",
      "177000 ('D08_nat_49_133',) 7 7 1.0\n",
      "178000 ('D08_nat_17_207',) 7 7 1.0\n",
      "179000 ('D08_tex_2_158',) 7 7 0.84\n",
      "180000 ('D08_tex_25_28',) 7 7 0.9\n",
      "Yes_Class 13\n",
      "181000 ('D14_nat_45_132',) 13 13 1.0\n",
      "182000 ('D14_tex_17_146',) 13 13 0.8\n",
      "183000 ('D14_tex_29_33',) 13 1 0.69\n",
      "184000 ('D14_obj_36_165',) 13 28 1.0\n",
      "185000 ('D14_obj_36_27',) 13 10 0.5\n",
      "186000 ('D14_tex_17_236',) 13 26 0.96\n",
      "187000 ('D14_nat_17_193',) 13 13 1.0\n",
      "188000 ('D14_tex_30_47',) 13 24 0.85\n",
      "Yes_Class 6\n",
      "189000 ('D07_tex_30_34',) 6 27 0.98\n",
      "190000 ('D07_obj_30_109',) 6 27 1.0\n",
      "191000 ('D07_obj_5_55',) 6 6 1.0\n",
      "192000 ('D07_nat_49_97',) 6 6 1.0\n",
      "193000 ('D07_color_14_12',) 6 6 1.0\n",
      "194000 ('D07_tex_39_69',) 6 27 1.0\n",
      "195000 ('D07_obj_39_45',) 6 6 1.0\n",
      "196000 ('D07_nat_30_74',) 6 6 0.43\n",
      "Yes_Class 16\n",
      "197000 ('D17_tex_29_149',) 16 16 1.0\n",
      "198000 ('D17_nat_25_15',) 16 6 1.0\n",
      "199000 ('D17_tex_39_0',) 16 16 1.0\n",
      "200000 ('D17_nat_29_149',) 16 16 0.99\n",
      "201000 ('D17_color_14_203',) 16 16 1.0\n",
      "202000 ('D17_nat_1_246',) 16 6 1.0\n",
      "203000 ('D17_nat_39_55',) 16 16 1.0\n",
      "204000 ('D17_tex_30_241',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "205000 ('D27_obj_17_151',) 26 26 1.0\n",
      "206000 ('D27_tex_2_19',) 26 26 1.0\n",
      "207000 ('D27_obj_2_1',) 26 26 1.0\n",
      "208000 ('D27_nat_30_180',) 26 26 1.0\n",
      "209000 ('D27_tex_25_81',) 26 26 1.0\n",
      "210000 ('D27_nat_25_247',) 26 22 0.89\n",
      "211000 ('D27_obj_5_218',) 26 26 1.0\n",
      "212000 ('D27_nat_39_73',) 26 26 1.0\n",
      "Yes_Class 11\n",
      "213000 ('D12_nat_29_155',) 11 11 1.0\n",
      "214000 ('D12_tex_17_178',) 11 29 0.48\n",
      "215000 ('D12_nat_30_251',) 11 19 0.48\n",
      "216000 ('D12_nat_2_11',) 11 12 0.65\n",
      "217000 ('D12_tex_29_22',) 11 19 0.65\n",
      "218000 ('D12_tex_2_35',) 11 18 1.0\n",
      "219000 ('D12_obj_5_91',) 11 15 0.94\n",
      "220000 ('D12_nat_29_21',) 11 11 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221000 ('D12_nat_1_36',) 11 11 0.89\n",
      "Yes_Class 15\n",
      "222000 ('D16_nat_49_125',) 15 15 1.0\n",
      "223000 ('D16_nat_45_215',) 15 15 1.0\n",
      "224000 ('D16_obj_5_100',) 15 15 0.56\n",
      "225000 ('D16_obj_25_194',) 15 15 0.71\n",
      "226000 ('D16_tex_36_63',) 15 15 1.0\n",
      "227000 ('D16_nat_1_185',) 15 15 1.0\n",
      "228000 ('D16_obj_25_109',) 15 15 0.99\n",
      "229000 ('D16_tex_36_79',) 15 15 1.0\n",
      "Yes_Class 24\n",
      "230000 ('D25_nat_39_133',) 24 24 1.0\n",
      "231000 ('D25_tex_2_186',) 24 26 0.99\n",
      "232000 ('D25_color_14_226',) 24 17 1.0\n",
      "233000 ('D25_nat_39_37',) 24 1 0.69\n",
      "234000 ('D25_nat_9_14',) 24 24 1.0\n",
      "235000 ('D25_nat_29_14',) 24 24 1.0\n",
      "236000 ('D25_tex_1_38',) 24 24 1.0\n",
      "237000 ('D25_nat_30_7',) 24 24 1.0\n",
      "Yes_Class 8\n",
      "238000 ('D09_nat_25_181',) 8 8 1.0\n",
      "239000 ('D09_nat_49_139',) 8 8 1.0\n",
      "240000 ('D09_nat_36_131',) 8 8 1.0\n",
      "241000 ('D09_tex_7_35',) 8 8 1.0\n",
      "242000 ('D09_obj_1_8',) 8 8 1.0\n",
      "243000 ('D09_nat_45_21',) 8 8 1.0\n",
      "244000 ('D09_nat_25_174',) 8 8 0.97\n",
      "245000 ('D09_color_14_122',) 8 8 1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Similar\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Similar folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,imgs2,labels1,patch_filename) in enumerate(test_loader_similar):\n",
    "        c = labels1\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org, mat_img, target = imgs1.to(device),imgs2.to(device), labels1.to(device)\n",
    "        output, fc_feature = model(img_org,mat_img)\n",
    "        \n",
    "        output = F.softmax(fc_feature,dim=1)\n",
    "        \n",
    "\n",
    "        actual = target\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f12704",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Similar/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9076631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 8192 26 5674 85.25 0.97\n",
      "32 8192 28 6038 84.24 0.97\n",
      "32 8192 28 6316 88.11 0.98\n",
      "32 8192 9 1951 84.68 0.99\n",
      "32 8192 31 7381 93.01 0.99\n",
      "32 8192 24 5245 85.37 0.98\n",
      "32 8192 32 8134 99.29 1.0\n",
      "32 8192 31 6911 87.08 0.98\n",
      "32 8192 26 5940 89.24 0.98\n",
      "32 8192 31 6848 86.29 0.98\n",
      "32 8192 15 3205 83.46 0.98\n",
      "32 8192 31 6882 86.72 0.98\n",
      "32 8192 29 5857 78.89 0.97\n",
      "32 8192 31 7211 90.86 0.98\n",
      "32 8192 30 7079 92.17 0.98\n",
      "32 8192 22 4079 72.43 0.96\n",
      "32 8192 22 4908 87.14 0.97\n",
      "32 8192 23 3614 61.38 0.95\n",
      "32 8192 24 5732 93.29 0.99\n",
      "32 8192 28 6618 92.33 0.99\n",
      "32 8192 29 7192 96.88 1.0\n",
      "32 8192 26 5740 86.24 0.98\n",
      "32 8192 17 3312 76.1 0.95\n",
      "32 8192 30 6665 86.78 0.98\n",
      "32 8192 21 3902 72.58 0.96\n",
      "32 8192 25 3648 57.0 0.93\n",
      "32 8192 29 6887 92.77 0.99\n",
      "32 8192 20 3402 66.45 0.94\n",
      "32 8192 31 6788 85.53 0.97\n",
      "32 8192 21 4833 89.9 0.98\n",
      "Image Level Accuracy: 80.20833333333334%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "      df = pd.read_csv(f)\n",
    "      data = df.sort_values(by=['Patch_Filename'])\n",
    "      classname = Path(f).stem\n",
    "      classname = int(classname.split(\"_\")[2])\n",
    "      a=\"a\"\n",
    "\n",
    "      true_image_class = classname\n",
    "      total_images_perclass = 0\n",
    "      correct_images_perclass = 0\n",
    "      total_patches_perclass = 0\n",
    "\n",
    "      total_class_votes =0\n",
    "      total_class_patches =0\n",
    "      prob_avg_correct_patch = 0.0\n",
    "      votes = 0\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "      for ind in data.index:\n",
    "              filename = df['Patch_Filename'][ind]\n",
    "              pred_patch_class = df['Predicted Class'][ind]\n",
    "              pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "              \n",
    "              filename = filename.split(\"_\")\n",
    "              file_length = len(filename)\n",
    "              initial_filename = filename[:-1]\n",
    "              patch_name = filename[file_length-1]\n",
    "              #print(initial_filename)\n",
    "              #print(patch_name)\n",
    "\n",
    "              if(a!= initial_filename and a==\"a\"):\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a!=initial_filename and a!=\"a\"):\n",
    "                  total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                  counts = np.bincount(arr_pred_patches)\n",
    "                  pred_image_class = np.argmax(counts)\n",
    "                  votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                  s=0\n",
    "                  \n",
    "                  if(pred_image_class == true_image_class):\n",
    "                      correct_images_perclass = correct_images_perclass + 1\n",
    "                      total_class_votes = total_class_votes + votes\n",
    "                      total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                      \n",
    "                      total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                      z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                      for i in range(0,len(arr_pred_patches)):\n",
    "                          if(z[i]==1):\n",
    "                              s= s+1\n",
    "                              prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "              \n",
    "                  arr_pred_patches = []\n",
    "                  arr_pred_patches_prob = []\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a==initial_filename):\n",
    "                  total_patches_perclass = total_patches_perclass + 1\n",
    "                  arr_pred_patches.append(pred_patch_class)\n",
    "                  arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "      total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "      counts = np.bincount(arr_pred_patches)\n",
    "      pred_image_class = np.argmax(counts)\n",
    "      votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "      if(pred_image_class == true_image_class):\n",
    "          correct_images_perclass = correct_images_perclass + 1\n",
    "          total_class_votes = total_class_votes + votes\n",
    "          total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "          \n",
    "          total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "          z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "          for i in range(0,len(arr_pred_patches)):\n",
    "              if(z[i]==1):\n",
    "                  prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      \n",
    "      if(correct_images_perclass!=0):\n",
    "          prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "          prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "          avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "          print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "      else:\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "              \n",
    "              \n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results.csv'))\n",
    "\n",
    "ILA_similar = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_similar}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abfdca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing Results_Merged folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n",
      "/tmp/ipykernel_26637/248763931.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  filters = torch.FloatTensor(filters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes_Class 0\n",
      "0 ('D01_rnd_34_15',) 0 0 1.0\n",
      "1000 ('D01_nat_39_179',) 0 0 0.94\n",
      "2000 ('D01_rnd_160_33',) 0 0 1.0\n",
      "3000 ('D01_color_2_66',) 0 15 1.0\n",
      "4000 ('D01_rnd_141_63',) 0 0 1.0\n",
      "5000 ('D01_tex_7_230',) 0 13 1.0\n",
      "6000 ('D01_rnd_99_27',) 0 0 1.0\n",
      "7000 ('D01_tex_29_111',) 0 0 1.0\n",
      "8000 ('D01_nat_30_173',) 0 13 1.0\n",
      "9000 ('D01_tex_7_224',) 0 0 1.0\n",
      "10000 ('D01_obj_29_123',) 0 29 0.72\n",
      "11000 ('D01_obj_29_155',) 0 3 0.5\n",
      "12000 ('D01_tex_17_89',) 0 0 0.88\n",
      "13000 ('D01_rnd_84_69',) 0 0 1.0\n",
      "14000 ('D01_nat_1_47',) 0 0 1.0\n",
      "15000 ('D01_rnd_86_150',) 0 0 1.0\n",
      "16000 ('D01_obj_39_197',) 0 0 0.99\n",
      "Yes_Class 28\n",
      "17000 ('D29_obj_25_106',) 28 3 0.77\n",
      "18000 ('D29_rnd_141_230',) 28 28 1.0\n",
      "19000 ('D29_tex_25_227',) 28 28 1.0\n",
      "20000 ('D29_rnd_98_217',) 28 28 1.0\n",
      "21000 ('D29_rnd_7_240',) 28 28 1.0\n",
      "22000 ('D29_rnd_122_207',) 28 28 1.0\n",
      "23000 ('D29_rnd_83_76',) 28 28 1.0\n",
      "24000 ('D29_nat_9_192',) 28 28 1.0\n",
      "25000 ('D29_obj_2_179',) 28 28 0.94\n",
      "26000 ('D29_rnd_98_202',) 28 28 1.0\n",
      "27000 ('D29_nat_1_160',) 28 28 1.0\n",
      "28000 ('D29_rnd_124_173',) 28 28 1.0\n",
      "29000 ('D29_obj_5_75',) 28 3 0.86\n",
      "30000 ('D29_rnd_19_187',) 28 28 1.0\n",
      "31000 ('D29_rnd_153_167',) 28 28 1.0\n",
      "32000 ('D29_tex_17_105',) 28 28 1.0\n",
      "33000 ('D29_nat_39_255',) 28 28 1.0\n",
      "34000 ('D29_nat_30_43',) 28 5 1.0\n",
      "Yes_Class 22\n",
      "35000 ('D23_rnd_124_222',) 22 22 1.0\n",
      "36000 ('D23_rnd_98_255',) 22 22 1.0\n",
      "37000 ('D23_nat_1_12',) 22 22 1.0\n",
      "38000 ('D23_rnd_81_128',) 22 22 1.0\n",
      "39000 ('D23_rnd_18_59',) 22 22 0.99\n",
      "40000 ('D23_nat_45_116',) 22 22 1.0\n",
      "41000 ('D23_nat_25_12',) 22 22 1.0\n",
      "42000 ('D23_rnd_99_151',) 22 22 1.0\n",
      "43000 ('D23_rnd_66_18',) 22 22 1.0\n",
      "44000 ('D23_rnd_66_145',) 22 22 1.0\n",
      "45000 ('D23_rnd_98_13',) 22 22 1.0\n",
      "46000 ('D23_rnd_138_191',) 22 22 1.0\n",
      "47000 ('D23_rnd_61_181',) 22 22 1.0\n",
      "48000 ('D23_rnd_138_175',) 22 22 0.99\n",
      "49000 ('D23_nat_1_135',) 22 14 0.83\n",
      "Yes_Class 27\n",
      "50000 ('D28_nat_9_126',) 27 27 1.0\n",
      "51000 ('D28_rnd_135_6',) 27 27 0.99\n",
      "52000 ('D28_tex_30_99',) 27 5 1.0\n",
      "53000 ('D28_rnd_65_75',) 27 27 1.0\n",
      "54000 ('D28_nat_30_231',) 27 27 1.0\n",
      "55000 ('D28_rnd_130_7',) 27 27 1.0\n",
      "56000 ('D28_tex_25_74',) 27 27 0.92\n",
      "57000 ('D28_rnd_124_234',) 27 28 1.0\n",
      "58000 ('D28_rnd_40_243',) 27 27 1.0\n",
      "59000 ('D28_rnd_40_100',) 27 27 1.0\n",
      "60000 ('D28_rnd_132_137',) 27 27 0.73\n",
      "61000 ('D28_obj_2_97',) 27 28 0.51\n",
      "62000 ('D28_nat_2_209',) 27 6 0.98\n",
      "63000 ('D28_rnd_65_218',) 27 27 1.0\n",
      "64000 ('D28_rnd_49_97',) 27 27 1.0\n",
      "65000 ('D28_nat_1_145',) 27 27 1.0\n",
      "66000 ('D28_rnd_17_149',) 27 27 1.0\n",
      "Yes_Class 3\n",
      "67000 ('D04_color_4_253',) 3 3 1.0\n",
      "68000 ('D04_rnd_150_132',) 3 3 1.0\n",
      "69000 ('D04_tex_2_185',) 3 1 0.74\n",
      "70000 ('D04_rnd_238_135',) 3 3 1.0\n",
      "71000 ('D04_tex_36_10',) 3 3 1.0\n",
      "72000 ('D04_rnd_205_123',) 3 3 1.0\n",
      "73000 ('D04_rnd_119_192',) 3 3 1.0\n",
      "74000 ('D04_rnd_188_116',) 3 3 1.0\n",
      "75000 ('D04_rnd_177_81',) 3 3 1.0\n",
      "76000 ('D04_rnd_141_121',) 3 3 1.0\n",
      "77000 ('D04_tex_36_35',) 3 3 1.0\n",
      "78000 ('D04_rnd_108_24',) 3 3 1.0\n",
      "79000 ('D04_tex_2_73',) 3 3 1.0\n",
      "80000 ('D04_nat_36_101',) 3 3 1.0\n",
      "81000 ('D04_rnd_24_254',) 3 3 1.0\n",
      "82000 ('D04_nat_9_60',) 3 3 1.0\n",
      "83000 ('D04_rnd_72_163',) 3 3 1.0\n",
      "84000 ('D04_rnd_226_185',) 3 3 1.0\n",
      "85000 ('D04_obj_2_117',) 3 3 0.98\n",
      "86000 ('D04_tex_29_155',) 3 3 1.0\n",
      "87000 ('D04_rnd_202_229',) 3 3 1.0\n",
      "Yes_Class 14\n",
      "88000 ('D15_obj_5_192',) 14 14 1.0\n",
      "89000 ('D15_rnd_221_30',) 14 14 1.0\n",
      "90000 ('D15_rnd_166_36',) 14 14 1.0\n",
      "91000 ('D15_rnd_178_2',) 14 14 1.0\n",
      "92000 ('D15_nat_9_66',) 14 14 1.0\n",
      "93000 ('D15_rnd_194_129',) 14 14 1.0\n",
      "94000 ('D15_rnd_196_75',) 14 14 1.0\n",
      "95000 ('D15_rnd_5_45',) 14 14 1.0\n",
      "96000 ('D15_rnd_41_148',) 14 14 1.0\n",
      "97000 ('D15_rnd_221_93',) 14 14 1.0\n",
      "98000 ('D15_rnd_111_50',) 14 14 1.0\n",
      "99000 ('D15_obj_30_12',) 14 14 1.0\n",
      "100000 ('D15_rnd_87_246',) 14 14 1.0\n",
      "101000 ('D15_tex_29_86',) 14 14 1.0\n",
      "102000 ('D15_rnd_99_32',) 14 14 1.0\n",
      "103000 ('D15_obj_5_28',) 14 14 1.0\n",
      "104000 ('D15_rnd_271_36',) 14 14 1.0\n",
      "105000 ('D15_color_14_129',) 14 14 1.0\n",
      "106000 ('D15_rnd_50_81',) 14 14 0.96\n",
      "107000 ('D15_rnd_41_170',) 14 14 1.0\n",
      "108000 ('D15_tex_29_12',) 14 14 1.0\n",
      "109000 ('D15_rnd_255_125',) 14 14 1.0\n",
      "Yes_Class 20\n",
      "110000 ('D21_rnd_110_64',) 20 20 1.0\n",
      "111000 ('D21_rnd_67_117',) 20 20 1.0\n",
      "112000 ('D21_nat_17_88',) 20 10 0.55\n",
      "113000 ('D21_tex_29_147',) 20 20 0.54\n",
      "114000 ('D21_rnd_99_143',) 20 20 1.0\n",
      "115000 ('D21_rnd_87_65',) 20 20 1.0\n",
      "116000 ('D21_rnd_65_55',) 20 20 1.0\n",
      "117000 ('D21_nat_17_57',) 20 10 0.99\n",
      "118000 ('D21_rnd_108_218',) 20 20 1.0\n",
      "119000 ('D21_rnd_51_10',) 20 20 1.0\n",
      "120000 ('D21_rnd_81_55',) 20 20 1.0\n",
      "121000 ('D21_tex_1_216',) 20 20 0.98\n",
      "122000 ('D21_tex_25_68',) 20 20 1.0\n",
      "123000 ('D21_rnd_81_194',) 20 20 0.9\n",
      "124000 ('D21_tex_1_247',) 20 10 1.0\n",
      "125000 ('D21_nat_17_42',) 20 1 0.96\n",
      "Yes_Class 4\n",
      "126000 ('D05_obj_5_235',) 4 4 0.99\n",
      "127000 ('D05_rnd_89_83',) 4 4 1.0\n",
      "128000 ('D05_rnd_54_36',) 4 4 1.0\n",
      "129000 ('D05_tex_36_24',) 4 20 0.47\n",
      "130000 ('D05_nat_29_94',) 4 4 1.0\n",
      "131000 ('D05_nat_49_16',) 4 10 0.81\n",
      "132000 ('D05_rnd_35_157',) 4 4 1.0\n",
      "133000 ('D05_rnd_35_32',) 4 4 1.0\n",
      "134000 ('D05_nat_9_85',) 4 4 1.0\n",
      "135000 ('D05_obj_30_176',) 4 20 0.93\n",
      "136000 ('D05_rnd_89_24',) 4 4 1.0\n",
      "137000 ('D05_rnd_135_166',) 4 4 1.0\n",
      "138000 ('D05_obj_17_9',) 4 4 1.0\n",
      "139000 ('D05_nat_45_36',) 4 4 1.0\n",
      "140000 ('D05_rnd_84_177',) 4 4 1.0\n",
      "141000 ('D05_obj_1_248',) 4 4 1.0\n",
      "142000 ('D05_tex_25_216',) 4 4 1.0\n",
      "Yes_Class 18\n",
      "143000 ('D19_obj_2_177',) 18 17 1.0\n",
      "144000 ('D19_rnd_150_152',) 18 18 1.0\n",
      "145000 ('D19_rnd_130_144',) 18 18 1.0\n",
      "146000 ('D19_rnd_108_61',) 18 18 1.0\n",
      "147000 ('D19_rnd_5_111',) 18 18 1.0\n",
      "148000 ('D19_rnd_65_81',) 18 18 1.0\n",
      "149000 ('D19_rnd_205_16',) 18 18 1.0\n",
      "150000 ('D19_rnd_160_184',) 18 18 1.0\n",
      "151000 ('D19_rnd_166_121',) 18 18 1.0\n",
      "152000 ('D19_rnd_108_245',) 18 18 1.0\n",
      "153000 ('D19_rnd_113_182',) 18 18 1.0\n",
      "154000 ('D19_rnd_110_247',) 18 18 1.0\n",
      "155000 ('D19_rnd_153_147',) 18 18 1.0\n",
      "156000 ('D19_rnd_122_77',) 18 18 1.0\n",
      "157000 ('D19_nat_36_195',) 18 24 0.83\n",
      "158000 ('D19_nat_45_81',) 18 18 1.0\n",
      "159000 ('D19_rnd_160_131',) 18 18 1.0\n",
      "160000 ('D19_rnd_150_22',) 18 18 1.0\n",
      "161000 ('D19_rnd_10_145',) 18 18 1.0\n",
      "162000 ('D19_nat_39_76',) 18 18 1.0\n",
      "Yes_Class 17\n",
      "163000 ('D18_rnd_150_110',) 17 17 1.0\n",
      "164000 ('D18_rnd_141_102',) 17 17 1.0\n",
      "165000 ('D18_obj_2_6',) 17 17 1.0\n",
      "166000 ('D18_rnd_141_56',) 17 17 1.0\n",
      "167000 ('D18_rnd_119_62',) 17 17 1.0\n",
      "168000 ('D18_tex_7_119',) 17 17 0.98\n",
      "169000 ('D18_tex_39_145',) 17 17 1.0\n",
      "170000 ('D18_rnd_41_120',) 17 17 1.0\n",
      "171000 ('D18_rnd_135_238',) 17 17 1.0\n",
      "172000 ('D18_obj_2_41',) 17 17 1.0\n",
      "173000 ('D18_rnd_57_46',) 17 17 1.0\n",
      "174000 ('D18_color_2_99',) 17 1 1.0\n",
      "175000 ('D18_obj_17_249',) 17 17 1.0\n",
      "176000 ('D18_tex_1_242',) 17 17 0.81\n",
      "177000 ('D18_nat_30_201',) 17 17 1.0\n",
      "178000 ('D18_obj_5_12',) 17 17 1.0\n",
      "Yes_Class 10\n",
      "179000 ('D11_obj_17_8',) 10 29 1.0\n",
      "180000 ('D11_rnd_10_51',) 10 10 1.0\n",
      "181000 ('D11_rnd_146_170',) 10 10 1.0\n",
      "182000 ('D11_nat_17_152',) 10 10 1.0\n",
      "183000 ('D11_color_4_53',) 10 10 1.0\n",
      "184000 ('D11_rnd_30_236',) 10 10 1.0\n",
      "185000 ('D11_tex_17_250',) 10 10 0.61\n",
      "186000 ('D11_rnd_21_244',) 10 10 1.0\n",
      "187000 ('D11_nat_29_227',) 10 10 1.0\n",
      "188000 ('D11_rnd_153_56',) 10 10 1.0\n",
      "189000 ('D11_tex_7_29',) 10 10 1.0\n",
      "190000 ('D11_rnd_42_163',) 10 10 1.0\n",
      "191000 ('D11_tex_17_87',) 10 10 1.0\n",
      "192000 ('D11_rnd_99_86',) 10 10 1.0\n",
      "193000 ('D11_nat_17_25',) 10 10 1.0\n",
      "194000 ('D11_rnd_30_141',) 10 10 1.0\n",
      "195000 ('D11_color_2_146',) 10 0 0.89\n",
      "Yes_Class 12\n",
      "196000 ('D13_rnd_44_110',) 12 12 1.0\n",
      "197000 ('D13_rnd_153_154',) 12 12 1.0\n",
      "198000 ('D13_color_14_157',) 12 12 1.0\n",
      "199000 ('D13_obj_39_229',) 12 12 1.0\n",
      "200000 ('D13_rnd_153_237',) 12 12 1.0\n",
      "201000 ('D13_rnd_146_63',) 12 12 1.0\n",
      "202000 ('D13_tex_2_113',) 12 9 0.96\n",
      "203000 ('D13_obj_30_152',) 12 19 1.0\n",
      "204000 ('D13_rnd_160_233',) 12 12 0.73\n",
      "205000 ('D13_rnd_130_84',) 12 12 1.0\n",
      "206000 ('D13_rnd_153_3',) 12 12 1.0\n",
      "207000 ('D13_rnd_122_56',) 12 12 1.0\n",
      "208000 ('D13_color_2_15',) 12 12 1.0\n",
      "209000 ('D13_color_4_67',) 12 12 0.96\n",
      "210000 ('D13_rnd_23_194',) 12 12 1.0\n",
      "211000 ('D13_rnd_50_99',) 12 12 1.0\n",
      "212000 ('D13_rnd_99_203',) 12 12 1.0\n",
      "Yes_Class 19\n",
      "213000 ('D20_tex_2_212',) 19 1 1.0\n",
      "214000 ('D20_rnd_43_212',) 19 19 1.0\n",
      "215000 ('D20_color_2_174',) 19 19 1.0\n",
      "216000 ('D20_rnd_135_246',) 19 19 1.0\n",
      "217000 ('D20_nat_2_236',) 19 19 1.0\n",
      "218000 ('D20_nat_30_201',) 19 19 1.0\n",
      "219000 ('D20_obj_39_96',) 19 19 1.0\n",
      "220000 ('D20_obj_30_178',) 19 19 1.0\n",
      "221000 ('D20_rnd_31_84',) 19 19 1.0\n",
      "222000 ('D20_obj_17_212',) 19 19 0.84\n",
      "223000 ('D20_rnd_160_208',) 19 19 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224000 ('D20_rnd_82_180',) 19 19 1.0\n",
      "225000 ('D20_tex_17_16',) 19 19 0.99\n",
      "226000 ('D20_tex_39_10',) 19 19 1.0\n",
      "227000 ('D20_rnd_110_149',) 19 19 0.99\n",
      "228000 ('D20_rnd_70_132',) 19 19 1.0\n",
      "229000 ('D20_rnd_130_191',) 19 19 1.0\n",
      "Yes_Class 5\n",
      "230000 ('D06_rnd_50_56',) 5 5 1.0\n",
      "231000 ('D06_nat_29_180',) 5 26 1.0\n",
      "232000 ('D06_rnd_50_101',) 5 5 1.0\n",
      "233000 ('D06_color_14_106',) 5 5 1.0\n",
      "234000 ('D06_nat_30_102',) 5 27 0.82\n",
      "235000 ('D06_rnd_132_81',) 5 5 1.0\n",
      "236000 ('D06_rnd_129_67',) 5 5 1.0\n",
      "237000 ('D06_rnd_16_193',) 5 5 1.0\n",
      "238000 ('D06_nat_36_45',) 5 6 0.99\n",
      "239000 ('D06_obj_30_181',) 5 27 1.0\n",
      "240000 ('D06_rnd_99_102',) 5 5 1.0\n",
      "241000 ('D06_tex_7_85',) 5 5 0.96\n",
      "242000 ('D06_rnd_64_143',) 5 5 1.0\n",
      "243000 ('D06_rnd_138_162',) 5 5 0.97\n",
      "244000 ('D06_rnd_53_101',) 5 5 1.0\n",
      "245000 ('D06_tex_2_255',) 5 6 0.49\n",
      "Yes_Class 1\n",
      "246000 ('D02_rnd_215_196',) 1 1 1.0\n",
      "247000 ('D02_nat_49_108',) 1 1 1.0\n",
      "248000 ('D02_nat_29_164',) 1 1 1.0\n",
      "249000 ('D02_rnd_215_20',) 1 1 1.0\n",
      "250000 ('D02_rnd_99_135',) 1 1 1.0\n",
      "251000 ('D02_rnd_177_123',) 1 1 1.0\n",
      "252000 ('D02_tex_2_233',) 1 1 1.0\n",
      "253000 ('D02_nat_1_184',) 1 1 1.0\n",
      "254000 ('D02_obj_29_87',) 1 1 1.0\n",
      "255000 ('D02_tex_30_82',) 1 20 1.0\n",
      "256000 ('D02_rnd_61_25',) 1 1 1.0\n",
      "257000 ('D02_rnd_7_45',) 1 1 1.0\n",
      "258000 ('D02_rnd_113_141',) 1 1 1.0\n",
      "259000 ('D02_obj_17_22',) 1 1 1.0\n",
      "260000 ('D02_tex_1_102',) 1 1 0.85\n",
      "261000 ('D02_tex_7_156',) 1 17 0.5\n",
      "262000 ('D02_rnd_70_124',) 1 1 1.0\n",
      "263000 ('D02_rnd_80_102',) 1 24 0.84\n",
      "264000 ('D02_rnd_215_129',) 1 1 1.0\n",
      "Yes_Class 2\n",
      "265000 ('D03_nat_2_111',) 2 2 0.91\n",
      "266000 ('D03_obj_2_220',) 2 2 0.95\n",
      "267000 ('D03_rnd_177_28',) 2 2 1.0\n",
      "268000 ('D03_rnd_77_242',) 2 2 1.0\n",
      "269000 ('D03_rnd_32_183',) 2 2 1.0\n",
      "270000 ('D03_rnd_187_233',) 2 2 1.0\n",
      "271000 ('D03_rnd_205_234',) 2 2 1.0\n",
      "272000 ('D03_rnd_130_34',) 2 2 1.0\n",
      "273000 ('D03_nat_25_216',) 2 2 1.0\n",
      "274000 ('D03_rnd_176_232',) 2 2 1.0\n",
      "275000 ('D03_rnd_202_155',) 2 2 1.0\n",
      "276000 ('D03_rnd_177_140',) 2 2 1.0\n",
      "277000 ('D03_tex_7_126',) 2 2 1.0\n",
      "278000 ('D03_tex_2_95',) 2 2 1.0\n",
      "279000 ('D03_rnd_166_222',) 2 2 1.0\n",
      "280000 ('D03_nat_17_111',) 2 2 1.0\n",
      "281000 ('D03_rnd_117_203',) 2 2 1.0\n",
      "282000 ('D03_rnd_31_96',) 2 2 1.0\n",
      "283000 ('D03_rnd_63_183',) 2 2 1.0\n",
      "284000 ('D03_obj_25_50',) 2 2 1.0\n",
      "Yes_Class 21\n",
      "285000 ('D22_rnd_5_209',) 21 21 1.0\n",
      "286000 ('D22_rnd_64_77',) 21 14 0.53\n",
      "287000 ('D22_color_14_198',) 21 13 0.83\n",
      "288000 ('D22_tex_30_42',) 21 21 1.0\n",
      "289000 ('D22_rnd_23_96',) 21 21 1.0\n",
      "290000 ('D22_nat_36_128',) 21 21 0.96\n",
      "291000 ('D22_tex_1_176',) 21 21 1.0\n",
      "292000 ('D22_rnd_110_198',) 21 21 1.0\n",
      "293000 ('D22_nat_36_22',) 21 21 1.0\n",
      "294000 ('D22_rnd_6_9',) 21 21 1.0\n",
      "295000 ('D22_rnd_5_156',) 21 21 1.0\n",
      "296000 ('D22_rnd_16_128',) 21 21 1.0\n",
      "297000 ('D22_tex_7_206',) 21 21 1.0\n",
      "298000 ('D22_rnd_132_48',) 21 21 1.0\n",
      "299000 ('D22_color_2_25',) 21 8 1.0\n",
      "300000 ('D22_color_14_130',) 21 8 1.0\n",
      "301000 ('D22_color_4_222',) 21 21 1.0\n",
      "Yes_Class 23\n",
      "302000 ('D24_color_4_113',) 23 13 1.0\n",
      "303000 ('D24_rnd_44_151',) 23 23 1.0\n",
      "304000 ('D24_nat_45_207',) 23 23 1.0\n",
      "305000 ('D24_nat_17_99',) 23 23 1.0\n",
      "306000 ('D24_nat_25_159',) 23 23 1.0\n",
      "307000 ('D24_rnd_10_166',) 23 23 1.0\n",
      "308000 ('D24_tex_7_158',) 23 26 0.9\n",
      "309000 ('D24_nat_49_143',) 23 22 0.93\n",
      "310000 ('D24_nat_39_250',) 23 23 1.0\n",
      "311000 ('D24_rnd_110_30',) 23 23 1.0\n",
      "312000 ('D24_tex_39_255',) 23 23 0.94\n",
      "313000 ('D24_obj_39_192',) 23 29 0.92\n",
      "314000 ('D24_obj_1_87',) 23 29 0.43\n",
      "315000 ('D24_tex_7_223',) 23 29 0.98\n",
      "316000 ('D24_rnd_54_71',) 23 23 1.0\n",
      "317000 ('D24_rnd_42_158',) 23 23 1.0\n",
      "Yes_Class 25\n",
      "318000 ('D26_obj_2_215',) 25 25 0.83\n",
      "319000 ('D26_obj_17_89',) 25 25 1.0\n",
      "320000 ('D26_rnd_150_37',) 25 25 1.0\n",
      "321000 ('D26_rnd_59_96',) 25 25 1.0\n",
      "322000 ('D26_tex_36_97',) 25 25 1.0\n",
      "323000 ('D26_obj_25_202',) 25 25 1.0\n",
      "324000 ('D26_tex_7_97',) 25 25 1.0\n",
      "325000 ('D26_nat_9_161',) 25 12 1.0\n",
      "326000 ('D26_obj_17_139',) 25 25 0.87\n",
      "327000 ('D26_nat_49_169',) 25 25 1.0\n",
      "328000 ('D26_color_4_21',) 25 25 1.0\n",
      "329000 ('D26_obj_30_72',) 25 25 1.0\n",
      "330000 ('D26_rnd_129_217',) 25 25 1.0\n",
      "331000 ('D26_obj_36_1',) 25 15 0.54\n",
      "332000 ('D26_color_4_83',) 25 25 1.0\n",
      "333000 ('D26_color_2_6',) 25 25 1.0\n",
      "334000 ('D26_tex_39_194',) 25 25 1.0\n",
      "Yes_Class 9\n",
      "335000 ('D10_rnd_129_41',) 9 9 1.0\n",
      "336000 ('D10_rnd_117_100',) 9 9 1.0\n",
      "337000 ('D10_tex_17_241',) 9 9 1.0\n",
      "338000 ('D10_rnd_110_209',) 9 9 1.0\n",
      "339000 ('D10_obj_1_237',) 9 9 1.0\n",
      "340000 ('D10_obj_29_127',) 9 9 1.0\n",
      "341000 ('D10_rnd_43_42',) 9 9 1.0\n",
      "342000 ('D10_nat_1_251',) 9 9 1.0\n",
      "343000 ('D10_rnd_32_242',) 9 9 1.0\n",
      "344000 ('D10_rnd_146_231',) 9 9 1.0\n",
      "345000 ('D10_obj_30_52',) 9 9 1.0\n",
      "346000 ('D10_rnd_122_239',) 9 9 1.0\n",
      "347000 ('D10_rnd_52_87',) 9 9 1.0\n",
      "348000 ('D10_obj_30_47',) 9 9 1.0\n",
      "349000 ('D10_tex_30_195',) 9 9 1.0\n",
      "350000 ('D10_obj_30_4',) 9 9 1.0\n",
      "Yes_Class 29\n",
      "351000 ('D30_nat_39_117',) 29 29 1.0\n",
      "352000 ('D30_obj_29_225',) 29 29 0.98\n",
      "353000 ('D30_color_4_82',) 29 10 0.7\n",
      "354000 ('D30_rnd_79_173',) 29 29 1.0\n",
      "355000 ('D30_nat_17_54',) 29 29 1.0\n",
      "356000 ('D30_rnd_28_86',) 29 29 1.0\n",
      "357000 ('D30_obj_17_135',) 29 29 1.0\n",
      "358000 ('D30_rnd_10_90',) 29 29 1.0\n",
      "359000 ('D30_rnd_99_166',) 29 29 1.0\n",
      "360000 ('D30_obj_29_103',) 29 29 1.0\n",
      "361000 ('D30_rnd_150_186',) 29 29 1.0\n",
      "362000 ('D30_rnd_135_242',) 29 29 1.0\n",
      "363000 ('D30_nat_17_15',) 29 29 1.0\n",
      "364000 ('D30_tex_7_227',) 29 29 1.0\n",
      "365000 ('D30_rnd_130_183',) 29 29 1.0\n",
      "366000 ('D30_rnd_119_91',) 29 29 1.0\n",
      "367000 ('D30_tex_39_166',) 29 15 0.51\n",
      "Yes_Class 7\n",
      "368000 ('D08_tex_29_18',) 7 7 1.0\n",
      "369000 ('D08_rnd_119_252',) 7 7 1.0\n",
      "370000 ('D08_rnd_108_107',) 7 7 1.0\n",
      "371000 ('D08_nat_29_173',) 7 7 1.0\n",
      "372000 ('D08_rnd_64_162',) 7 7 1.0\n",
      "373000 ('D08_tex_30_64',) 7 7 1.0\n",
      "374000 ('D08_nat_17_98',) 7 7 1.0\n",
      "375000 ('D08_rnd_153_161',) 7 7 0.97\n",
      "376000 ('D08_nat_39_145',) 7 7 1.0\n",
      "377000 ('D08_obj_17_163',) 7 7 1.0\n",
      "378000 ('D08_rnd_187_148',) 7 7 1.0\n",
      "379000 ('D08_rnd_110_61',) 7 7 1.0\n",
      "380000 ('D08_rnd_124_213',) 7 7 1.0\n",
      "381000 ('D08_rnd_2_83',) 7 18 0.87\n",
      "382000 ('D08_rnd_113_65',) 7 7 1.0\n",
      "383000 ('D08_rnd_141_49',) 7 7 1.0\n",
      "384000 ('D08_obj_2_208',) 7 7 0.65\n",
      "385000 ('D08_obj_17_145',) 7 7 1.0\n",
      "Yes_Class 13\n",
      "386000 ('D14_rnd_146_117',) 13 13 1.0\n",
      "387000 ('D14_rnd_124_42',) 13 13 1.0\n",
      "388000 ('D14_rnd_146_146',) 13 13 1.0\n",
      "389000 ('D14_nat_2_139',) 13 13 1.0\n",
      "390000 ('D14_tex_36_238',) 13 13 1.0\n",
      "391000 ('D14_nat_29_143',) 13 13 1.0\n",
      "392000 ('D14_obj_2_223',) 13 24 0.96\n",
      "393000 ('D14_nat_30_206',) 13 13 0.93\n",
      "394000 ('D14_rnd_3_209',) 13 13 1.0\n",
      "395000 ('D14_tex_7_62',) 13 20 1.0\n",
      "396000 ('D14_rnd_38_218',) 13 13 0.95\n",
      "397000 ('D14_nat_2_245',) 13 13 1.0\n",
      "398000 ('D14_obj_2_219',) 13 28 0.43\n",
      "399000 ('D14_rnd_130_128',) 13 9 0.55\n",
      "400000 ('D14_rnd_38_82',) 13 13 1.0\n",
      "401000 ('D14_rnd_29_154',) 13 14 0.99\n",
      "402000 ('D14_rnd_119_156',) 13 13 1.0\n",
      "Yes_Class 6\n",
      "403000 ('D07_rnd_119_109',) 6 6 1.0\n",
      "404000 ('D07_obj_39_148',) 6 6 0.66\n",
      "405000 ('D07_rnd_27_18',) 6 6 0.92\n",
      "406000 ('D07_color_14_27',) 6 6 1.0\n",
      "407000 ('D07_color_14_62',) 6 5 0.97\n",
      "408000 ('D07_obj_17_247',) 6 5 0.94\n",
      "409000 ('D07_rnd_119_145',) 6 6 1.0\n",
      "410000 ('D07_color_2_238',) 6 6 1.0\n",
      "411000 ('D07_rnd_66_251',) 6 6 1.0\n",
      "412000 ('D07_obj_25_13',) 6 6 0.99\n",
      "413000 ('D07_nat_9_40',) 6 6 1.0\n",
      "414000 ('D07_nat_29_41',) 6 6 1.0\n",
      "415000 ('D07_obj_36_203',) 6 28 0.98\n",
      "416000 ('D07_nat_36_189',) 6 6 1.0\n",
      "417000 ('D07_tex_30_150',) 6 6 0.99\n",
      "418000 ('D07_rnd_108_196',) 6 6 1.0\n",
      "419000 ('D07_rnd_55_41',) 6 6 1.0\n",
      "420000 ('D07_rnd_166_247',) 6 6 1.0\n",
      "Yes_Class 16\n",
      "421000 ('D17_tex_17_11',) 16 16 1.0\n",
      "422000 ('D17_obj_17_203',) 16 16 1.0\n",
      "423000 ('D17_rnd_4_143',) 16 16 1.0\n",
      "424000 ('D17_rnd_138_142',) 16 16 1.0\n",
      "425000 ('D17_obj_25_241',) 16 27 0.81\n",
      "426000 ('D17_obj_2_93',) 16 28 1.0\n",
      "427000 ('D17_rnd_130_107',) 16 16 1.0\n",
      "428000 ('D17_tex_7_192',) 16 16 1.0\n",
      "429000 ('D17_color_4_67',) 16 16 1.0\n",
      "430000 ('D17_nat_36_123',) 16 6 1.0\n",
      "431000 ('D17_rnd_68_87',) 16 16 1.0\n",
      "432000 ('D17_tex_2_150',) 16 16 1.0\n",
      "433000 ('D17_rnd_117_126',) 16 16 1.0\n",
      "434000 ('D17_rnd_48_165',) 16 16 1.0\n",
      "435000 ('D17_color_4_141',) 16 16 1.0\n",
      "436000 ('D17_tex_25_234',) 16 16 1.0\n",
      "Yes_Class 26\n",
      "437000 ('D27_nat_25_168',) 26 13 0.7\n",
      "438000 ('D27_nat_9_167',) 26 26 1.0\n",
      "439000 ('D27_rnd_113_52',) 26 26 1.0\n",
      "440000 ('D27_rnd_75_14',) 26 26 0.94\n",
      "441000 ('D27_nat_39_226',) 26 26 1.0\n",
      "442000 ('D27_obj_36_10',) 26 26 1.0\n",
      "443000 ('D27_obj_2_87',) 26 15 0.77\n",
      "444000 ('D27_obj_17_248',) 26 26 1.0\n",
      "445000 ('D27_rnd_146_23',) 26 26 1.0\n",
      "446000 ('D27_obj_1_122',) 26 26 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447000 ('D27_rnd_108_239',) 26 26 1.0\n",
      "448000 ('D27_obj_1_140',) 26 26 1.0\n",
      "449000 ('D27_rnd_36_34',) 26 26 1.0\n",
      "450000 ('D27_rnd_25_224',) 26 26 0.99\n",
      "451000 ('D27_rnd_75_19',) 26 20 0.8\n",
      "452000 ('D27_tex_17_145',) 26 26 1.0\n",
      "Yes_Class 11\n",
      "453000 ('D12_tex_30_62',) 11 15 0.99\n",
      "454000 ('D12_rnd_46_6',) 11 11 1.0\n",
      "455000 ('D12_tex_29_80',) 11 29 0.97\n",
      "456000 ('D12_obj_2_231',) 11 28 0.92\n",
      "457000 ('D12_nat_45_232',) 11 11 1.0\n",
      "458000 ('D12_rnd_36_106',) 11 11 1.0\n",
      "459000 ('D12_rnd_36_185',) 11 11 1.0\n",
      "460000 ('D12_rnd_188_5',) 11 11 1.0\n",
      "461000 ('D12_obj_5_62',) 11 12 0.99\n",
      "462000 ('D12_rnd_99_17',) 11 11 1.0\n",
      "463000 ('D12_rnd_57_59',) 11 11 1.0\n",
      "464000 ('D12_rnd_160_123',) 11 11 1.0\n",
      "465000 ('D12_rnd_138_82',) 11 11 1.0\n",
      "466000 ('D12_rnd_86_0',) 11 11 1.0\n",
      "467000 ('D12_rnd_71_16',) 11 11 1.0\n",
      "468000 ('D12_tex_7_0',) 11 26 0.82\n",
      "469000 ('D12_tex_25_149',) 11 11 0.97\n",
      "470000 ('D12_tex_30_85',) 11 17 0.88\n",
      "Yes_Class 15\n",
      "471000 ('D16_rnd_46_112',) 15 15 1.0\n",
      "472000 ('D16_nat_39_158',) 15 15 1.0\n",
      "473000 ('D16_tex_7_51',) 15 15 1.0\n",
      "474000 ('D16_rnd_129_16',) 15 15 1.0\n",
      "475000 ('D16_tex_25_213',) 15 15 1.0\n",
      "476000 ('D16_obj_1_219',) 15 0 0.54\n",
      "477000 ('D16_rnd_108_107',) 15 15 1.0\n",
      "478000 ('D16_rnd_141_7',) 15 15 1.0\n",
      "479000 ('D16_rnd_124_83',) 15 15 1.0\n",
      "480000 ('D16_rnd_73_132',) 15 15 1.0\n",
      "481000 ('D16_rnd_73_79',) 15 15 1.0\n",
      "482000 ('D16_rnd_130_223',) 15 15 1.0\n",
      "483000 ('D16_nat_39_197',) 15 15 1.0\n",
      "484000 ('D16_color_2_188',) 15 15 1.0\n",
      "485000 ('D16_rnd_117_251',) 15 15 1.0\n",
      "486000 ('D16_tex_2_165',) 15 15 1.0\n",
      "Yes_Class 24\n",
      "487000 ('D25_rnd_135_243',) 24 24 1.0\n",
      "488000 ('D25_nat_29_57',) 24 17 0.59\n",
      "489000 ('D25_obj_29_13',) 24 15 0.56\n",
      "490000 ('D25_rnd_124_189',) 24 24 1.0\n",
      "491000 ('D25_rnd_77_46',) 24 24 1.0\n",
      "492000 ('D25_nat_2_238',) 24 24 1.0\n",
      "493000 ('D25_rnd_65_10',) 24 24 1.0\n",
      "494000 ('D25_rnd_38_2',) 24 24 1.0\n",
      "495000 ('D25_rnd_22_120',) 24 24 1.0\n",
      "496000 ('D25_tex_36_20',) 24 17 1.0\n",
      "497000 ('D25_nat_29_211',) 24 24 1.0\n",
      "498000 ('D25_rnd_47_108',) 24 24 1.0\n",
      "499000 ('D25_rnd_153_248',) 24 24 1.0\n",
      "500000 ('D25_obj_30_150',) 24 24 1.0\n",
      "501000 ('D25_nat_36_190',) 24 24 1.0\n",
      "502000 ('D25_rnd_39_219',) 24 24 1.0\n",
      "503000 ('D25_rnd_38_102',) 24 24 1.0\n",
      "Yes_Class 8\n",
      "504000 ('D09_rnd_110_182',) 8 8 1.0\n",
      "505000 ('D09_rnd_25_22',) 8 8 1.0\n",
      "506000 ('D09_obj_29_153',) 8 8 1.0\n",
      "507000 ('D09_nat_9_36',) 8 8 1.0\n",
      "508000 ('D09_nat_36_169',) 8 8 0.95\n",
      "509000 ('D09_rnd_113_229',) 8 8 1.0\n",
      "510000 ('D09_rnd_63_225',) 8 8 1.0\n",
      "511000 ('D09_rnd_89_107',) 8 8 1.0\n",
      "512000 ('D09_obj_29_7',) 8 8 1.0\n",
      "513000 ('D09_color_4_176',) 8 8 1.0\n",
      "514000 ('D09_obj_29_39',) 8 8 1.0\n",
      "515000 ('D09_obj_2_42',) 8 8 0.97\n",
      "516000 ('D09_rnd_24_204',) 8 8 1.0\n",
      "517000 ('D09_rnd_108_32',) 8 8 1.0\n",
      "518000 ('D09_tex_7_100',) 8 8 1.0\n",
      "519000 ('D09_nat_29_75',) 8 8 1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "results_folder = \"Results_Merged\"\n",
    "if os.path.exists(results_folder):\n",
    "    # If it exists, delete the folder and its content\n",
    "    print(\"Deleting existing Results_Merged folder...\")\n",
    "    for file in os.listdir(results_folder):\n",
    "        file_path = os.path.join(results_folder, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "a = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,imgs2,labels1,patch_filename) in enumerate(test_loader_merged):\n",
    "        c = labels1\n",
    "        d = c.cpu().numpy()[0]\n",
    "\n",
    "        if(d!=a):\n",
    "            print(\"Yes_Class\",d)\n",
    "            a= d\n",
    "            z = d\n",
    "            file_class = os.path.join(results_folder, f\"Test_Class_{z}.csv\")\n",
    "\n",
    "            with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Batch_Id\",\"Patch_Filename\",\"True Class\",\"Predicted Class\",\"Probability of Predicted Class\"])\n",
    "    \n",
    "\n",
    "\n",
    "        img_org, mat_img, target = imgs1.to(device),imgs2.to(device), labels1.to(device)\n",
    "        output, fc_feature = model(img_org,mat_img)\n",
    "        \n",
    "        output = F.softmax(fc_feature,dim=1)\n",
    "        \n",
    "\n",
    "        actual = target\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        y_true = actual.cpu().numpy()[0]\n",
    "        y_pred =predicted.cpu().numpy()[0]\n",
    "\n",
    "        prob_y_pred = output[0][y_pred]\n",
    "        prob_y_pred = prob_y_pred.cpu().numpy()\n",
    "        prob_y_pred = np.around(prob_y_pred,decimals=2)\n",
    "\n",
    "        if(batch_idx % 1000 == 0):\n",
    "          print(batch_idx,patch_filename,y_true,y_pred,prob_y_pred)\n",
    "        \n",
    "        \n",
    "        with open(file_class, 'a+', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch_idx,patch_filename,y_true,y_pred,prob_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1638f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = img_dir = \"Results_Merged/\"\n",
    "data_path = os.path.join(img_dir,'*csv')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "with open(os.path.join(csv_dir, 'Image_Level_Results.csv'), 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Class Label\",\"Number of Images\",\\\n",
    "                         \"Correct Predicted Images\",\\\n",
    "                         \"Number of Patches\",\\\n",
    "                         \"Total Patches(Correct Classified Images)\",\\\n",
    "                         \"Correct Predicted Patches(Correct Classified Images)\",\\\n",
    "                         \"Precetange Votes Per Image(Only Correct Images)\",\\\n",
    "                         \"Average Softmax Probability of Correct Patch(Only Correct Images)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c47df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 17408 62 14217 89.57 0.98\n",
      "64 16384 60 14043 91.43 0.99\n",
      "75 19200 71 17250 94.91 0.99\n",
      "70 17920 46 11217 95.25 0.99\n",
      "64 16384 63 15194 94.21 0.99\n",
      "66 16896 58 13768 92.73 0.99\n",
      "64 16384 64 16091 98.21 1.0\n",
      "72 18432 70 16728 93.35 0.99\n",
      "76 19456 70 16682 93.09 0.99\n",
      "79 20224 78 18639 93.34 0.99\n",
      "67 17152 48 11348 92.35 0.99\n",
      "63 16128 62 14567 91.78 0.99\n",
      "66 16896 63 14091 87.37 0.98\n",
      "66 16896 65 15675 94.2 0.99\n",
      "65 16640 63 15467 95.9 0.99\n",
      "63 16128 53 11408 84.08 0.98\n",
      "66 16896 56 13371 93.27 0.99\n",
      "63 16128 54 10769 77.9 0.97\n",
      "63 16128 55 13563 96.33 1.0\n",
      "88 22528 84 20431 95.01 0.99\n",
      "83 21248 80 20222 98.74 1.0\n",
      "65 16640 59 13731 90.91 0.99\n",
      "63 16128 47 10350 86.02 0.97\n",
      "65 16640 63 14738 91.38 0.99\n",
      "63 16128 48 9820 79.92 0.97\n",
      "62 15872 55 10637 75.55 0.97\n",
      "64 16384 60 14503 94.42 0.99\n",
      "64 16384 52 11345 85.22 0.98\n",
      "67 17152 65 15092 90.7 0.98\n",
      "67 17152 55 13258 94.16 0.99\n",
      "Image Level Accuracy: 90.05416051206302%\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "      df = pd.read_csv(f)\n",
    "      data = df.sort_values(by=['Patch_Filename'])\n",
    "      classname = Path(f).stem\n",
    "      classname = int(classname.split(\"_\")[2])\n",
    "      a=\"a\"\n",
    "\n",
    "      true_image_class = classname\n",
    "      total_images_perclass = 0\n",
    "      correct_images_perclass = 0\n",
    "      total_patches_perclass = 0\n",
    "\n",
    "      total_class_votes =0\n",
    "      total_class_patches =0\n",
    "      prob_avg_correct_patch = 0.0\n",
    "      votes = 0\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      total_correc_img_patches = 0\n",
    "\n",
    "\n",
    "      for ind in data.index:\n",
    "              filename = df['Patch_Filename'][ind]\n",
    "              pred_patch_class = df['Predicted Class'][ind]\n",
    "              pred_patch_prob = df['Probability of Predicted Class'][ind]\n",
    "              \n",
    "              filename = filename.split(\"_\")\n",
    "              file_length = len(filename)\n",
    "              initial_filename = filename[:-1]\n",
    "              patch_name = filename[file_length-1]\n",
    "              #print(initial_filename)\n",
    "              #print(patch_name)\n",
    "\n",
    "              if(a!= initial_filename and a==\"a\"):\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a!=initial_filename and a!=\"a\"):\n",
    "                  total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "                  counts = np.bincount(arr_pred_patches)\n",
    "                  pred_image_class = np.argmax(counts)\n",
    "                  votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "                  s=0\n",
    "                  \n",
    "                  if(pred_image_class == true_image_class):\n",
    "                      correct_images_perclass = correct_images_perclass + 1\n",
    "                      total_class_votes = total_class_votes + votes\n",
    "                      total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "                      \n",
    "                      total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "                      z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "                      for i in range(0,len(arr_pred_patches)):\n",
    "                          if(z[i]==1):\n",
    "                              s= s+1\n",
    "                              prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "              \n",
    "                  arr_pred_patches = []\n",
    "                  arr_pred_patches_prob = []\n",
    "                  a = initial_filename\n",
    "\n",
    "\n",
    "\n",
    "              if(a==initial_filename):\n",
    "                  total_patches_perclass = total_patches_perclass + 1\n",
    "                  arr_pred_patches.append(pred_patch_class)\n",
    "                  arr_pred_patches_prob.append(pred_patch_prob)\n",
    "\n",
    "\n",
    "\n",
    "      total_images_perclass = total_images_perclass + 1\n",
    "\n",
    "      counts = np.bincount(arr_pred_patches)\n",
    "      pred_image_class = np.argmax(counts)\n",
    "      votes = np.count_nonzero(arr_pred_patches==pred_image_class)\n",
    "\n",
    "      if(pred_image_class == true_image_class):\n",
    "          correct_images_perclass = correct_images_perclass + 1\n",
    "          total_class_votes = total_class_votes + votes\n",
    "          total_class_patches = total_class_patches + len(arr_pred_patches)\n",
    "          \n",
    "          total_correc_img_patches = total_correc_img_patches + len(arr_pred_patches)\n",
    "\n",
    "          z = np.where(np.array(arr_pred_patches)==true_image_class,1,0)\n",
    "          for i in range(0,len(arr_pred_patches)):\n",
    "              if(z[i]==1):\n",
    "                  prob_avg_correct_patch = prob_avg_correct_patch + arr_pred_patches_prob[i]\n",
    "\n",
    "      arr_pred_patches = []\n",
    "      arr_pred_patches_prob = []\n",
    "      \n",
    "      if(correct_images_perclass!=0):\n",
    "          prob_avg_correct_patch = prob_avg_correct_patch / total_class_votes\n",
    "          prob_avg_correct_patch = np.around(prob_avg_correct_patch,decimals=2)\n",
    "          avg_vote_perclass = np.around((total_class_votes*100) / total_correc_img_patches,decimals=2)\n",
    "          print(total_images_perclass,total_patches_perclass,correct_images_perclass,total_class_votes,avg_vote_perclass,prob_avg_correct_patch)  \n",
    "\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "      else:\n",
    "          with open(os.path.join(csv_dir + 'Image_Level_Results.csv'), 'a+', newline='') as file:\n",
    "              writer = csv.writer(file)\n",
    "              writer.writerow([classname,total_images_perclass,correct_images_perclass,total_patches_perclass,\\\n",
    "                              total_correc_img_patches,total_class_votes,\\\n",
    "                              avg_vote_perclass,\\\n",
    "                              prob_avg_correct_patch])\n",
    "              \n",
    "              \n",
    "df = pd.read_csv(os.path.join(csv_dir + 'Image_Level_Results.csv'))\n",
    "\n",
    "ILA_merged = sum(df['Correct Predicted Images']) / sum(df['Number of Images']) * 100\n",
    "\n",
    "print(f\"Image Level Accuracy: {ILA_merged}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62697619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Level Accuracy (Trained on Random Set, Tested on Random Set): 98.8795518207283%\n",
      "Image Level Accuracy (Trained on Random Set, Tested on Similar Set): 80.20833333333334%\n",
      "Image Level Accuracy (Trained on Random Set, Tested on Merged Set): 90.05416051206302%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Random Set): {ILA_random}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Similar Set): {ILA_similar}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Merged Set): {ILA_merged}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5023e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
