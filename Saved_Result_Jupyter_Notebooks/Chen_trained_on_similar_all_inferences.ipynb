{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc122d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.2+cu121\n",
      "Torchvision Version:  0.16.2+cu121\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torchvision import models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce775ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "   '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "   '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP','.mat',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "   return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "   classes = os.listdir(dir)\n",
    "   classes.sort()\n",
    "   class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "   return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "   images = []\n",
    "   for target in os.listdir(dir):\n",
    "       d = os.path.join(dir, target)\n",
    "       if not os.path.isdir(d):\n",
    "           continue\n",
    "\n",
    "       for filename in os.listdir(d):\n",
    "           if is_image_file(filename):\n",
    "               path = '{0}/{1}'.format(target, filename)\n",
    "               #print(path)\n",
    "               item = (path, class_to_idx[target])\n",
    "               images.append(item)\n",
    "\n",
    "   return images\n",
    "\n",
    "def default_loader(path):\n",
    "   return Image.open(path).convert('RGB')\n",
    "\n",
    "def mat_loader(path):\n",
    "   return scipy.io.loadmat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a221eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 1071\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 960\n",
      "====================================================================================================\n",
      "{'D01_Samsung_Galaxy_S20Plus': 0, 'D02_Nothing_One': 1, 'D03_Samsung_Galaxy_A03': 2, 'D04_Samsung_Galaxy_M04': 3, 'D05_Vivo_V9_Pro': 4, 'D06_Apple_Iphone_12Mini': 5, 'D07_Apple_Iphone_11': 6, 'D08_Redmi_Note_8Pro': 7, 'D09_Samsung_Galaxy_J8_10G': 8, 'D10_Samsung_Galaxy_F41': 9, 'D11_OnePlus_8T': 10, 'D12_Vivo_Y02t': 11, 'D13_Oppo_A17k': 12, 'D14_Samsung_Galaxy_S20FE': 13, 'D15_Motorola_Motog60': 14, 'D16_Samsung_Galaxy_S21FE': 15, 'D17_Apple_Iphone_12': 16, 'D18_IQOO_Z3': 17, 'D19_IQOO_Z6_Lite': 18, 'D20_Motorola_MotoG73_5G': 19, 'D21_OnePlus_10Pro_5G': 20, 'D22_Poco_F5': 21, 'D23_Poco_F5_Pro_5G': 22, 'D24_Realme_8': 23, 'D25_Realme_X3_Superzoom': 24, 'D26_Redmi_9i_Sport': 25, 'D27_Redmi_Note10_Pro': 26, 'D28_Apple_Iphone_13': 27, 'D29_Apple_Iphone_15': 28, 'D30_Vivo_Y75': 29} 2031\n"
     ]
    }
   ],
   "source": [
    "classes1, class_to_idx1 = find_classes(\"/home/user1/icip/similar/train/\")\n",
    "       \n",
    "imgs1 = make_dataset(\"/home/user1/icip/random/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/similar/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "imgs1 = make_dataset(\"/home/user1/icip/merged/test/\", class_to_idx1)\n",
    "print(class_to_idx1,len(imgs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99ab9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderLoader(data.Dataset):\n",
    "   def __init__(self, root1,transform_1=None,\n",
    "                target_transform=None,\n",
    "                loader=default_loader):\n",
    "       classes1, class_to_idx1 = find_classes(root1)\n",
    "       \n",
    "       imgs1 = make_dataset(root1, class_to_idx1)\n",
    "      \n",
    "\n",
    "       self.root1 = root1\n",
    "       self.imgs1 = imgs1\n",
    "       self.classes1 = classes1\n",
    "       self.class_to_idx1 = class_to_idx1\n",
    "       self.transform_1 = transform_1\n",
    "       self.target_transform = target_transform\n",
    "       self.loader = loader\n",
    "        \n",
    "       self.img_noise = None\n",
    "       self.img_rgb = None\n",
    "    \n",
    "    \n",
    "   def __getitem__(self, index):\n",
    "    \n",
    "\n",
    "       path1, target1 = self.imgs1[index]    \n",
    "       filename = Path(path1).stem \n",
    "       img1 = self.loader(os.path.join(self.root1, path1))  \n",
    "    \n",
    "       self.img_rgb = img1\n",
    "       \n",
    "\n",
    "       if self.transform_1 is not None:\n",
    "           img1 = self.transform_1(self.img_rgb)\n",
    "        \n",
    "       if self.target_transform is not None:\n",
    "           target1 = self.target_transform(target)\n",
    "            \n",
    "       \n",
    "       return img1,target1\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.imgs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688767a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        resnet1 = models.resnet50(pretrained=True)\n",
    "        modules1 = list(resnet1.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet1 = nn.Sequential(*modules1)\n",
    "\n",
    "    \n",
    "    \n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc2 = nn.Linear(2048,30)\n",
    "        \n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.resnet1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "       \n",
    "        out_fc = x\n",
    "        output = self.logsoftmax(x)\n",
    "        \n",
    "        return output, out_fc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61ac55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "transforms.Resize((256,256)),\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "batchsize=1\n",
    "\n",
    "val_dataset_random = ImageFolderLoader(\n",
    "        \"/home/user1/icip/random/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_random = torch.utils.data.DataLoader(\n",
    "        val_dataset_random, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_similar = ImageFolderLoader(\n",
    "        \"/home/user1/icip/similar/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_similar = torch.utils.data.DataLoader(\n",
    "        val_dataset_similar, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )\n",
    "\n",
    "val_dataset_merged = ImageFolderLoader(\n",
    "        \"/home/user1/icip/merged/test/\",\n",
    "        transform_1=data_transforms\n",
    "    )\n",
    "\n",
    "test_loader_merged = torch.utils.data.DataLoader(\n",
    "        val_dataset_merged, batch_size=batchsize,\n",
    "        shuffle=False, num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7856095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 960, 2031)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset_random), len(val_dataset_similar), len(val_dataset_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61908949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user1/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b151cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 0 [0/1071 (0%)]\tLoss: 6.729555\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [200/1071 (19%)]\tLoss: 0.235904\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [400/1071 (37%)]\tLoss: 0.000349\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [600/1071 (56%)]\tLoss: 6.979759\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [800/1071 (75%)]\tLoss: 0.127837\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [1000/1071 (93%)]\tLoss: 8.957326\n",
      "Accuracy=== 0.0\n",
      "6024.908671816747\n",
      "\n",
      "Image Level Accuracy: 32.96%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"Chen_trained_model_similar_final\")\n",
    "model.eval()\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "test_loss = 0\n",
    "total_loss = 0\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "test_epoch = 1\n",
    "total_correct_test = 0\n",
    "total_correct_train = 0\n",
    "total_correct = 0\n",
    "epoch = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,labels1) in enumerate(test_loader_random):\n",
    "        \n",
    "        img_org,target = imgs1.to(device),labels1.to(device)\n",
    "        output, fc_feature = model(img_org)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        test_loss = test_loss + loss.item()  # sum up batch loss\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total = float(len(target))\n",
    "        correct = (predicted == target).sum()\n",
    "        total_correct_test = total_correct_test + np.float64(correct.cpu().numpy())\n",
    "\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(img_org), len(test_loader_random.dataset),\n",
    "                100. * batch_idx / len(test_loader_random), loss.item()))\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            accuracy = 100.0 * float(correct) / total\n",
    "            print(\"Accuracy===\",accuracy)\n",
    "\n",
    "\n",
    "alpha = (len(test_loader_random.dataset))/ batchsize\n",
    "print(test_loss)\n",
    "test_loss /= alpha\n",
    "\n",
    "ILA_random=np.around((100* total_correct_test / len(test_loader_random.dataset)),decimals=2)\n",
    "\n",
    "\n",
    "print('\\nImage Level Accuracy: {}%\\n'.format(ILA_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3d2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 0 [0/960 (0%)]\tLoss: 0.943109\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [200/960 (21%)]\tLoss: 0.000003\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [400/960 (42%)]\tLoss: 0.007061\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [600/960 (62%)]\tLoss: 0.000000\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [800/960 (83%)]\tLoss: 0.000021\n",
      "Accuracy=== 100.0\n",
      "1301.175138946286\n",
      "\n",
      "Image Level Accuracy: 80.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "total_loss = 0\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "test_epoch = 1\n",
    "total_correct_test = 0\n",
    "total_correct_train = 0\n",
    "total_correct = 0\n",
    "epoch = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,labels1) in enumerate(test_loader_similar):\n",
    "        \n",
    "        img_org,target = imgs1.to(device),labels1.to(device)\n",
    "        output, fc_feature = model(img_org)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        test_loss = test_loss + loss.item()  # sum up batch loss\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total = float(len(target))\n",
    "        correct = (predicted == target).sum()\n",
    "        total_correct_test = total_correct_test + np.float64(correct.cpu().numpy())\n",
    "\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(img_org), len(test_loader_similar.dataset),\n",
    "                100. * batch_idx / len(test_loader_similar), loss.item()))\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            accuracy = 100.0 * float(correct) / total\n",
    "            print(\"Accuracy===\",accuracy)\n",
    "\n",
    "\n",
    "alpha = (len(test_loader_similar.dataset))/ batchsize\n",
    "print(test_loss)\n",
    "test_loss /= alpha\n",
    "\n",
    "ILA_similar=np.around((100* total_correct_test / len(test_loader_similar.dataset)),decimals=2)\n",
    "\n",
    "\n",
    "print('\\nImage Level Accuracy: {}%\\n'.format(ILA_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deeb4c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 0 [0/2031 (0%)]\tLoss: 0.943109\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [200/2031 (10%)]\tLoss: 0.000104\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [400/2031 (20%)]\tLoss: 0.000029\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [600/2031 (30%)]\tLoss: 8.885495\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [800/2031 (39%)]\tLoss: 0.123875\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [1000/2031 (49%)]\tLoss: 7.128145\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [1200/2031 (59%)]\tLoss: 0.001118\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [1400/2031 (69%)]\tLoss: 0.000045\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [1600/2031 (79%)]\tLoss: 0.000000\n",
      "Accuracy=== 100.0\n",
      "Test Epoch: 0 [1800/2031 (89%)]\tLoss: 1.793527\n",
      "Accuracy=== 0.0\n",
      "Test Epoch: 0 [2000/2031 (98%)]\tLoss: 0.000000\n",
      "Accuracy=== 100.0\n",
      "7326.083810763034\n",
      "\n",
      "Image Level Accuracy: 55.24%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "total_loss = 0\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "test_epoch = 1\n",
    "total_correct_test = 0\n",
    "total_correct_train = 0\n",
    "total_correct = 0\n",
    "epoch = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs1,labels1) in enumerate(test_loader_merged):\n",
    "        \n",
    "        img_org,target = imgs1.to(device),labels1.to(device)\n",
    "        output, fc_feature = model(img_org)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        test_loss = test_loss + loss.item()  # sum up batch loss\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total = float(len(target))\n",
    "        correct = (predicted == target).sum()\n",
    "        total_correct_test = total_correct_test + np.float64(correct.cpu().numpy())\n",
    "\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(img_org), len(test_loader_merged.dataset),\n",
    "                100. * batch_idx / len(test_loader_merged), loss.item()))\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            accuracy = 100.0 * float(correct) / total\n",
    "            print(\"Accuracy===\",accuracy)\n",
    "\n",
    "\n",
    "alpha = (len(test_loader_merged.dataset))/ batchsize\n",
    "print(test_loss)\n",
    "test_loss /= alpha\n",
    "\n",
    "ILA_merged=np.around((100* total_correct_test / len(test_loader_merged.dataset)),decimals=2)\n",
    "\n",
    "\n",
    "print('\\nImage Level Accuracy: {}%\\n'.format(ILA_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335d8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Level Accuracy (Trained on Random Set, Tested on Random Set): 32.96%\n",
      "Image Level Accuracy (Trained on Random Set, Tested on Similar Set): 80.1%\n",
      "Image Level Accuracy (Trained on Random Set, Tested on Merged Set): 55.24%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Random Set): {ILA_random}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Similar Set): {ILA_similar}%\")\n",
    "print(f\"Image Level Accuracy (Trained on Random Set, Tested on Merged Set): {ILA_merged}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492fc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
